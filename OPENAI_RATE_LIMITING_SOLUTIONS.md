# Soluciones para OpenAI Rate Limiting (429 Too Many Requests)

**Problema**: El error `429 Too Many Requests` ocurre cuando se envÃ­an mÃºltiples archivos al mismo tiempo, causando muchas llamadas simultÃ¡neas a OpenAI.

**Causas**:
1. Metadata extraction para cada chunk (espera de respuesta OpenAI)
2. Embeddings para cada chunk (llamadas simultÃ¡neas)
3. Varias solicitudes concurrentes sin delay

---

## ğŸ¯ Soluciones Propuestas (Ordenadas por RecomendaciÃ³n)

### **SOLUCIÃ“N 1: Rate Limiting con Retry AutomÃ¡tico â­ RECOMENDADA**

**DescripciÃ³n**: Usar LangChain's built-in retry logic con exponential backoff.

**Ventajas**:
- âœ… AutomÃ¡tico y transparente
- âœ… Respeta rate limits de OpenAI
- âœ… No requiere cambios en lÃ³gica de negocio
- âœ… Maneja otros errores (timeouts, 429, etc)
- âœ… Mejor que delay fijo (mÃ¡s eficiente)

**Ventajas vs Delay Fijo**:
- Delay fijo = esperar siempre, aunque no lo necesites
- Retry automÃ¡tico = solo espera si OpenAI dice que esperes

**ImplementaciÃ³n**:

```python
# En metadata_handler.py y chain_builder.py

from langchain_core.utils.function_calling import convert_to_openai_function
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)
import httpx

# Decorator para reintentos automÃ¡ticos
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=2, min=2, max=60),  # 2s, 4s, 8s...
    retry=retry_if_exception_type(httpx.HTTPStatusError),
)
def extract_metadata_with_retry(self, text: str):
    return self.extract_metadata(text)

# O aplicar a nivel de LLM:
self.llm = ChatOpenAI(
    model=settings.openai_model,
    temperature=0,
    api_key=settings.openai_api_key,
    max_retries=3,  # Reintentos automÃ¡ticos
    timeout=30,
)
```

**Complejidad**: â­ Baja (1 parÃ¡metro)
**Costo**: Sin costo adicional
**Tiempo implementaciÃ³n**: 15 minutos

---

### **SOLUCIÃ“N 2: Rate Limiting con Delay Fijo (1 segundo)**

**DescripciÃ³n**: Agregar delay de 1 segundo entre llamadas a OpenAI.

**Ventajas**:
- âœ… Simple de implementar
- âœ… Predecible (siempre espera X segundos)
- âœ… Garantiza que no tendrÃ¡s 429

**Desventajas**:
- âŒ Ineficiente (esperas aunque no lo necesites)
- âŒ MÃ¡s lento overall (siempre suma 1s por llamada)
- âŒ No se adapta si OpenAI estÃ¡ mÃ¡s lento
- âŒ 10 archivos = mÃ­nimo 10 segundos

**ImplementaciÃ³n**:

```python
import time
from functools import wraps

def rate_limit_openai(delay_seconds=1.0):
    """Decorator que agrega delay entre llamadas a OpenAI"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            time.sleep(delay_seconds)
            return func(*args, **kwargs)
        return wrapper
    return decorator

# Usar en metadata_handler.py:
@rate_limit_openai(delay_seconds=1.0)
def extract_metadata(self, text: str):
    # ...
```

**Complejidad**: â­ Baja
**Costo**: Tiempo (mÃ¡s lento)
**Tiempo implementaciÃ³n**: 10 minutos

---

### **SOLUCIÃ“N 3: Batch Processing + Async**

**DescripciÃ³n**: Procesar archivos secuencialmente en lugar de paralelo, pero usar async para no bloquear.

**Ventajas**:
- âœ… No bloquea threads
- âœ… Escalable para mÃºltiples usuarios
- âœ… Eficiente en recursos

**Desventajas**:
- âŒ MÃ¡s complejo de implementar
- âŒ Requiere refactor significativo

**ImplementaciÃ³n**:

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def process_documents_sequentially(documents):
    """Procesa documentos uno por uno de forma asincrÃ³nica"""
    results = []
    for doc in documents:
        result = await asyncio.sleep(1.0)  # Rate limit
        result = await process_document(doc)
        results.append(result)
    return results
```

**Complejidad**: â­â­â­ Media-Alta
**Costo**: Sin costo adicional
**Tiempo implementaciÃ³n**: 1-2 horas

---

### **SOLUCIÃ“N 4: Deshabilitar Metadata Extraction**

**DescripciÃ³n**: OpciÃ³n de toggle para desactivar la extracciÃ³n de metadata que consume muchas llamadas a OpenAI.

**Ventajas**:
- âœ… Inmediato (sin 429)
- âœ… Reduce costos OpenAI
- âœ… MÃ¡s rÃ¡pido
- âœ… Usuario elige si quiere o no

**Desventajas**:
- âŒ Pierde informaciÃ³n semÃ¡ntica
- âŒ Afecta calidad de bÃºsqueda

**ImplementaciÃ³n**:

```python
# En config/settings.py:
ENABLE_METADATA_EXTRACTION: bool = Field(
    default=False,  # Desactivar por defecto
    description="Enable metadata extraction (uses OpenAI calls)"
)

# En api/main.py:
async def initialize(
    collection_name: str,
    documents: List[Dict],
    force_recreate: bool = False,
    enable_metadata: bool = False  # Parameter
):
    # Solo extraer metadata si estÃ¡ habilitado
```

**Complejidad**: â­ Baja
**Costo**: Sin costo
**Tiempo implementaciÃ³n**: 20 minutos

---

### **SOLUCIÃ“N 5: Redis Cache para Metadata**

**DescripciÃ³n**: Cachear metadata ya extraÃ­da para no repetir llamadas.

**Ventajas**:
- âœ… No re-procesa documentos iguales
- âœ… Reduce llamadas OpenAI
- âœ… RÃ¡pido para documentos conocidos

**Desventajas**:
- âŒ No ayuda con documentos nuevos
- âŒ Complejidad media

**ImplementaciÃ³n**:

```python
from src.services.cache import RedisCache

class MetadataHandlerCached:
    def __init__(self):
        self.cache = RedisCache()
        self.handler = MetadataHandler()

    def extract_metadata(self, text: str) -> Dict:
        # Crear hash del texto
        cache_key = f"metadata:{hash(text)}"

        # Buscar en cache
        cached = self.cache.get(cache_key)
        if cached:
            return cached

        # Si no estÃ¡, extraer y cachear
        result = self.handler.extract_metadata(text)
        self.cache.set(cache_key, result, ttl=86400)  # 24 horas
        return result
```

**Complejidad**: â­â­ Media
**Costo**: Sin costo adicional (Redis ya estÃ¡)
**Tiempo implementaciÃ³n**: 45 minutos

---

### **SOLUCIÃ“N 6: Hybrid (Recomendado para ProducciÃ³n)**

**DescripciÃ³n**: Combinar retry automÃ¡tico + disable metadata + cache.

**Ventajas**:
- âœ… MÃ¡xima resiliencia
- âœ… Bajo costo
- âœ… RÃ¡pido
- âœ… Escalable

**ImplementaciÃ³n** (Orden de prioridad):

1. **Primero**: Retry automÃ¡tico en LLM (SOLUCIÃ“N 1)
2. **Segundo**: Deshabilitar metadata por defecto (SOLUCIÃ“N 4)
3. **Tercero**: Cachear cuando estÃ© habilitada (SOLUCIÃ“N 5)

**Tiempo total implementaciÃ³n**: 2 horas

---

## ğŸ“Š Comparativa de Soluciones

| SoluciÃ³n | Eficiencia | Complejidad | Tiempo | RecomendaciÃ³n |
|----------|-----------|-----------|--------|------|
| **1. Retry Auto** | â­â­â­â­â­ | â­ | 15min | âœ… INMEDIATO |
| **2. Delay Fijo** | â­â­ | â­ | 10min | âŒ Lento |
| **3. Async Batch** | â­â­â­â­ | â­â­â­ | 2h | Futuro |
| **4. Deshabilitar Meta** | â­â­â­ | â­ | 20min | âœ… Combinable |
| **5. Redis Cache** | â­â­â­ | â­â­ | 45min | âœ… Combinable |
| **6. Hybrid** | â­â­â­â­â­ | â­â­ | 2h | âœ…âœ… Ã“PTIMO |

---

## ğŸ¯ Mi RecomendaciÃ³n

**Para ahora (inmediato)**:
```
SOLUCIÃ“N 1: Agregar max_retries=3 al ChatOpenAI
â†’ 15 minutos
â†’ Resuelve el problema automÃ¡ticamente
â†’ Sin impacto en performance
```

**Para despuÃ©s (prÃ³xima iteraciÃ³n)**:
```
SOLUCIÃ“N 6: Hybrid approach
1. Retry automÃ¡tico âœ… (ya hecho)
2. + Deshabilitar metadata por defecto
3. + Cachear si estÃ¡ habilitada

â†’ 2 horas total
â†’ Sistema robusto y eficiente
â†’ Listo para producciÃ³n
```

---

## âœ… Mi RecomendaciÃ³n Final

**Implementar AHORA**:
- SOLUCIÃ“N 1 (Retry automÃ¡tico) - 15 minutos

**Implementar en Phase 6B**:
- SOLUCIÃ“N 4 + 5 + 1 (Hybrid) - 2 horas

Â¿CuÃ¡l prefieres que implemente primero?

