{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8dRP7-AdX2a"
   },
   "source": [
    "# RAG Avanzado 2025: De RecuperaciÃ³n Simple a Agentes Inteligentes\n",
    "\n",
    "## ğŸ¯ Objetivos de Aprendizaje\n",
    "\n",
    "Al finalizar esta clase, los estudiantes serÃ¡n capaces de:\n",
    "\n",
    "1. **Comprender la evoluciÃ³n del RAG** desde implementaciones bÃ¡sicas hasta sistemas agentic\n",
    "2. **Implementar RAG moderno** usando Qdrant Cloud y LangGraph\n",
    "3. **Dominar tÃ©cnicas avanzadas** como Self-RAG, Corrective RAG y Adaptive RAG\n",
    "4. **Integrar mÃºltiples fuentes de datos** (vectores, Excel/CSV, web search)\n",
    "5. **Comparar enfoques** entre OpenAI Responses API y LangGraph custom\n",
    "6. **Construir agentes RAG completos** con capacidades de toma de decisiones\n",
    "\n",
    "## ğŸ“‹ Ãndice del Contenido\n",
    "\n",
    "### **Parte I: Fundamentos Modernos**\n",
    "1. [IntroducciÃ³n al RAG en 2025](#introduccion)\n",
    "2. [ConfiguraciÃ³n del Entorno](#setup)\n",
    "3. [RAG BÃ¡sico con Qdrant Cloud](#rag-basico)\n",
    "\n",
    "### **Parte II: OrchestraciÃ³n Inteligente**\n",
    "4. [IntroducciÃ³n a LangGraph](#langgraph)\n",
    "5. [TÃ©cnicas RAG Avanzadas 2025](#tecnicas-avanzadas)\n",
    "6. [RAG Multi-Fuente](#multi-fuente)\n",
    "\n",
    "### **Parte III: Sistemas Agentic**\n",
    "7. [OpenAI Responses API](#openai-responses)\n",
    "8. [Agentic RAG Completo](#agentic-rag)\n",
    "9. [ComparaciÃ³n y Mejores PrÃ¡cticas](#comparacion)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Â¿QuÃ© es RAG en 2025?\n",
    "\n",
    "El **Retrieval-Augmented Generation (RAG)** ha evolucionado significativamente desde sus inicios. En 2025, RAG no es solo \"buscar y generar\", sino un **ecosistema inteligente** que:\n",
    "\n",
    "### **RAG Tradicional (2023)**\n",
    "```\n",
    "Usuario â†’ Embedding â†’ Vector DB â†’ Top-K â†’ LLM â†’ Respuesta\n",
    "```\n",
    "\n",
    "### **RAG Agentic (2025)**\n",
    "```\n",
    "Usuario â†’ Agente Inteligente â†’ AnÃ¡lisis de Consulta â†’ Router DinÃ¡mico\n",
    "                â†“\n",
    "[Vector DB | Web Search | Datos Estructurados | APIs]\n",
    "                â†“\n",
    "EvaluaciÃ³n de Relevancia â†’ CorrecciÃ³n AutomÃ¡tica â†’ GeneraciÃ³n Contextual\n",
    "```\n",
    "\n",
    "### **CaracterÃ­sticas Clave del RAG Moderno:**\n",
    "\n",
    "- **ğŸ§  Inteligencia Adaptativa**: El sistema decide dinÃ¡micamente quÃ© fuentes usar\n",
    "- **ğŸ”„ Auto-CorrecciÃ³n**: Detecta y corrige respuestas irrelevantes automÃ¡ticamente  \n",
    "- **ğŸ“Š Multi-Modal**: Combina texto, datos estructurados y bÃºsqueda web\n",
    "- **ğŸ¯ Context-Aware**: Mantiene estado y contexto a travÃ©s de conversaciones\n",
    "- **âš¡ Performance Optimizado**: BÃºsquedas paralelas y re-ranking inteligente\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Stack TecnolÃ³gico 2025\n",
    "\n",
    "| Componente | TecnologÃ­a | PropÃ³sito |\n",
    "|------------|------------|-----------|\n",
    "| **Vector Database** | Qdrant Cloud | Almacenamiento y bÃºsqueda semÃ¡ntica |\n",
    "| **OrchestraciÃ³n** | LangGraph | Workflows agentic y toma de decisiones |\n",
    "| **LLM Principal** | OpenAI GPT-4o | GeneraciÃ³n y razonamiento |\n",
    "| **Embeddings** | OpenAI text-embedding-3-large | RepresentaciÃ³n vectorial |\n",
    "| **Web Search** | OpenAI/Tavily | InformaciÃ³n en tiempo real |\n",
    "| **Structured Data** | Pandas + LangChain | Datos tabulares y CSV/Excel |\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“ Nota PedagÃ³gica**: Este notebook estÃ¡ diseÃ±ado para enseÃ±ar progresivamente desde conceptos bÃ¡sicos hasta implementaciones estado del arte. Cada secciÃ³n incluye teorÃ­a, cÃ³digo prÃ¡ctico y comparaciones con enfoques anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8143,
     "status": "ok",
     "timestamp": 1720877635396,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "yyiJccvpdYjU",
    "outputId": "1e67d712-e8b3-4cd0-e4ab-c13f9bbdd233"
   },
   "outputs": [],
   "source": [
    "# ğŸ“¦ InstalaciÃ³n de Dependencias 2025\n",
    "# Actualizamos las librerÃ­as para incluir las Ãºltimas versiones y nuevas herramientas\n",
    "\n",
    "# ğŸ“¦ InstalaciÃ³n de Dependencias 2025\n",
    "# En esta secciÃ³n listamos las dependencias necesarias para el entorno educativo de RAG.\n",
    "# Usamos Poetry para gestionar versiones y asegurar reproducibilidad.\n",
    "# Cada librerÃ­a estÃ¡ seleccionada por su relevancia en el stack moderno de RAG y agentes IA.\n",
    "\n",
    "# VersiÃ³n de Python recomendada para compatibilidad con las Ãºltimas librerÃ­as\n",
    "# python = \"^3.12\"\n",
    "\n",
    "# LangChain y extensiones para workflows RAG y agentes\n",
    "# langchain-openai = \"^0.3.28\"         # IntegraciÃ³n directa con OpenAI (embeddings y LLM)\n",
    "# langchain = \"^0.3.26\"                # Framework principal para RAG y chains\n",
    "# langchain-community = \"^0.3.27\"      # Integraciones con fuentes y herramientas externas\n",
    "# langchainhub = \"^0.1.21\"             # Acceso a prompts y chains predefinidos\n",
    "# langchain-qdrant = \"^0.2.0\"          # Conector para Qdrant Cloud como vector store\n",
    "# langchain-core = \"^0.3.72\"           # NÃºcleo de LangChain para chains y prompts\n",
    "# langchain-experimental = \"^0.3.4\"    # Funcionalidades experimentales (MultiQuery, HyDE, etc.)\n",
    "\n",
    "# Vector stores y bases de datos semÃ¡nticas\n",
    "# qdrant-client = \"^1.15.0\"            # Cliente oficial para Qdrant Cloud\n",
    "# faiss-cpu = \"^1.11.0.post1\"          # Alternativa local para bÃºsquedas vectoriales\n",
    "\n",
    "# Modelos y procesamiento de lenguaje\n",
    "# transformers = \"^4.53.3\"             # Modelos de HuggingFace para embeddings alternativos\n",
    "\n",
    "# VisualizaciÃ³n y anÃ¡lisis de datos\n",
    "# matplotlib = \"^3.10.3\"               # GrÃ¡ficos estÃ¡ticos\n",
    "# plotly = \"^6.2.0\"                    # GrÃ¡ficos interactivos\n",
    "# umap-learn = \"^0.5.9.post2\"          # ReducciÃ³n de dimensionalidad para visualizaciÃ³n de embeddings\n",
    "\n",
    "# ManipulaciÃ³n de datos estructurados\n",
    "# pandas = \"^2.3.1\"                    # AnÃ¡lisis y manipulaciÃ³n de datos tabulares\n",
    "# scikit-learn = \"^1.7.1\"              # Algoritmos de machine learning y mÃ©tricas\n",
    "\n",
    "# Utilidades para notebooks y entorno\n",
    "# python-dotenv = \"^1.0.1\"             # GestiÃ³n de variables de entorno (API keys)\n",
    "# nest-asyncio = \"^1.6.0\"              # Soporte para async en Jupyter\n",
    "# ipywidgets = \"^8.1.7\"                # Widgets interactivos en notebooks\n",
    "# nbformat = \"^5.10.4\"                 # Compatibilidad con formatos de notebook\n",
    "# openpyxl = \"^3.1.5\"                  # Lectura/escritura de archivos Excel\n",
    "\n",
    "# Web scraping y procesamiento de texto\n",
    "# beautifulsoup4 = \"^4.13.4\"           # ExtracciÃ³n de datos de pÃ¡ginas web\n",
    "\n",
    "# OrquestaciÃ³n avanzada de agentes\n",
    "# langgraph = \"^0.5.4\"                 # Framework para agentes y workflows complejos\n",
    "\n",
    "# Observabilidad y experiment tracking\n",
    "# langsmith = \"^0.4.8\"                 # Seguimiento de experimentos y debugging de chains\n",
    "\n",
    "# âœ… Estas dependencias cubren todo el ciclo de aprendizaje de RAG moderno:\n",
    "# - Chunking y embeddings\n",
    "# - Vector stores en la nube y local\n",
    "# - OrquestaciÃ³n agentic\n",
    "# - VisualizaciÃ³n y anÃ¡lisis\n",
    "# - IntegraciÃ³n con APIs y datos estructurados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq5-VGUHEWZV"
   },
   "source": [
    "# ğŸ”§ ConfiguraciÃ³n del Entorno\n",
    "\n",
    "## Variables de Entorno Necesarias\n",
    "\n",
    "Para este notebook necesitarÃ¡s configurar las siguientes API keys en tu archivo `.env`:\n",
    "\n",
    "```bash\n",
    "# OpenAI (para LLM y embeddings)\n",
    "OPENAI_API_KEY=tu_openai_key_aqui\n",
    "\n",
    "# Qdrant Cloud (para vector database)\n",
    "QDRANT_URL=tu_qdrant_cloud_url\n",
    "QDRANT_API_KEY=tu_qdrant_api_key\n",
    "\n",
    "```\n",
    "\n",
    "## ğŸ“‹ Servicios Necesarios\n",
    "\n",
    "### **1. OpenAI API**\n",
    "- Registrarse en: https://platform.openai.com/\n",
    "- Crear API key y aÃ±adir crÃ©ditos\n",
    "- Usaremos GPT-4o y text-embedding-3-large\n",
    "\n",
    "### **2. Qdrant Cloud**\n",
    "- Registrarse en: https://cloud.qdrant.io/\n",
    "- Crear cluster gratuito (1GB)\n",
    "- Obtener URL y API key del dashboard\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Tip**: En Google Colab, puedes usar `userdata.get('OPENAI_API_KEY')` en lugar de variables de entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zF_QWP7zESpO"
   },
   "outputs": [],
   "source": [
    "# ğŸ”‘ ConfiguraciÃ³n de Credenciales y Imports\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suprimir warnings para output mÃ¡s limpio\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Verificar que las credenciales estÃ©n configuradas\n",
    "required_keys = [\"OPENAI_API_KEY\", \"QDRANT_URL\", \"QDRANT_API_KEY\"]\n",
    "missing_keys = [key for key in required_keys if not os.getenv(key)]\n",
    "\n",
    "if missing_keys:\n",
    "    print(f\"âš ï¸  Faltan las siguientes variables de entorno: {missing_keys}\")\n",
    "    print(\"ğŸ“ ConfigÃºralas en tu archivo .env o en Google Colab userdata\")\n",
    "else:\n",
    "    print(\"âœ… Todas las credenciales estÃ¡n configuradas correctamente\")\n",
    "    \n",
    "# Test de conexiÃ³n bÃ¡sica\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    print(\"âœ… ConexiÃ³n con OpenAI establecida\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error conectando con OpenAI: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae8W66qmquBB"
   },
   "source": [
    "# ğŸ“Š GeneraciÃ³n de Datos SintÃ©ticos\n",
    "\n",
    "Para este tutorial usaremos datos sintÃ©ticos que simulan un escenario empresarial real. Generaremos:\n",
    "\n",
    "1. **ğŸ“„ Documentos de texto**: ArtÃ­culos sobre tecnologÃ­a e IA en espaÃ±ol\n",
    "2. **ğŸ“ˆ Datos estructurados**: CSV con mÃ©tricas empresariales  \n",
    "3. **ğŸ‘¥ InformaciÃ³n de empleados**: Excel con datos ficticios\n",
    "\n",
    "## Â¿Por quÃ© Datos SintÃ©ticos?\n",
    "\n",
    "- **ğŸ¯ PropÃ³sito educativo**: Datos controlados para demostrar conceptos especÃ­ficos\n",
    "- **ğŸ”’ Privacidad**: No usamos datos reales sensibles\n",
    "- **ğŸ¨ PersonalizaciÃ³n**: DiseÃ±ados para mostrar diferentes tÃ©cnicas RAG\n",
    "- **ğŸ“ Escalabilidad**: FÃ¡ciles de modificar segÃºn necesidades del curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1720877662346,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "5u4J9lBeF0EL",
    "outputId": "a0d96817-a6b2-46fb-fa1e-79cbab20be50"
   },
   "outputs": [],
   "source": [
    "# ğŸ“„ GeneraciÃ³n de Documentos de Texto\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Crear directorio para datos si no existe\n",
    "data_dir = Path(\"synthetic_data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Documentos sintÃ©ticos sobre tecnologÃ­a e IA\n",
    "synthetic_documents = [\n",
    "    {\n",
    "        \"title\": \"IntroducciÃ³n a la Inteligencia Artificial en 2025\",\n",
    "        \"content\": \"\"\"La Inteligencia Artificial en 2025 ha revolucionado mÃºltiples industrias. Los modelos de lenguaje grande (LLMs) como GPT-4 y Claude han democratizado el acceso a capacidades avanzadas de procesamiento de lenguaje natural. \n",
    "\n",
    "Las principales aplicaciones incluyen asistentes virtuales inteligentes, sistemas de recomendaciÃ³n personalizados, y herramientas de automatizaciÃ³n empresarial. La integraciÃ³n de IA en procesos de negocio ha mejorado la eficiencia operativa en un 40% promedio.\n",
    "\n",
    "Los desafÃ­os actuales incluyen la gestiÃ³n de sesgos algorÃ­tmicos, la interpretabilidad de modelos complejos, y el consumo energÃ©tico de los centros de datos. Las empresas estÃ¡n adoptando enfoques de IA responsable para mitigar estos riesgos.\"\"\",\n",
    "        \"category\": \"ai_general\",\n",
    "        \"date\": \"2025-01-15\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"RAG y RecuperaciÃ³n de InformaciÃ³n: Estado del Arte\",\n",
    "        \"content\": \"\"\"Retrieval-Augmented Generation (RAG) se ha consolidado como la tÃ©cnica mÃ¡s efectiva para combinar conocimiento externo con modelos de lenguaje. Los sistemas RAG modernos incorporan mÃºltiples fuentes de informaciÃ³n: bases de datos vectoriales, APIs externas, y bÃºsqueda web en tiempo real.\n",
    "\n",
    "Las tÃ©cnicas avanzadas incluyen Self-RAG para auto-evaluaciÃ³n de respuestas, Corrective RAG para correcciÃ³n automÃ¡tica de errores, y Adaptive RAG para enrutamiento inteligente de consultas. Estas mejoras han incrementado la precisiÃ³n de respuestas en un 60% comparado con implementaciones bÃ¡sicas.\n",
    "\n",
    "Los vectores embeddings mÃ¡s utilizados son text-embedding-3-large de OpenAI y el modelo e5-large-v2 de Microsoft, ambos optimizados para tareas de recuperaciÃ³n multilingÃ¼e.\"\"\",\n",
    "        \"category\": \"rag_technology\",\n",
    "        \"date\": \"2025-01-20\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Bases de Datos Vectoriales: Comparativa 2025\",\n",
    "        \"content\": \"\"\"Las bases de datos vectoriales han experimentado un crecimiento exponencial. Qdrant, Pinecone, Weaviate y Chroma lideran el mercado con diferentes fortalezas.\n",
    "\n",
    "Qdrant destaca por su rendimiento en bÃºsquedas hÃ­bridas (densas y dispersas), soporte nativo para filtros complejos, y escalabilidad horizontal. Su arquitectura permite manejar millones de vectores con latencias sub-100ms.\n",
    "\n",
    "Pinecone ofrece simplicidad de uso y infraestructura completamente gestionada, ideal para startups y MVPs. Weaviate se enfoca en capacidades de GraphQL y bÃºsquedas semÃ¡nticas complejas. Chroma prioriza la integraciÃ³n con herramientas de desarrollo local.\n",
    "\n",
    "La elecciÃ³n depende de factores como volumen de datos, latencia requerida, presupuesto y complejidad de consultas.\"\"\",\n",
    "        \"category\": \"vector_databases\",\n",
    "        \"date\": \"2025-01-25\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"LangGraph: OrchestraciÃ³n de Agentes IA\",\n",
    "        \"content\": \"\"\"LangGraph ha emergido como el framework lÃ­der para construir sistemas agentic complejos. Su arquitectura basada en grafos permite crear workflows con mÃºltiples nodos de decisiÃ³n, estados persistentes, y capacidades de recovery.\n",
    "\n",
    "Las principales ventajas incluyen debugging visual de flujos, checkpointing automÃ¡tico para recovery, y integraciÃ³n nativa con herramientas externas. Los desarrolladores pueden crear agentes que combinan llamadas a APIs, consultas a bases de datos, y razonamiento multi-paso.\n",
    "\n",
    "Casos de uso exitosos incluyen asistentes de investigaciÃ³n cientÃ­fica, sistemas de anÃ¡lisis financiero automatizado, y chatbots empresariales con acceso a mÃºltiples fuentes de datos. La curva de aprendizaje es moderada comparada con frameworks mÃ¡s complejos.\"\"\",\n",
    "        \"category\": \"langgraph\",\n",
    "        \"date\": \"2025-02-01\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"PolÃ­ticas de IA Empresarial y Governance\",\n",
    "        \"content\": \"\"\"La governance de IA se ha vuelto crÃ­tica para organizaciones que implementan sistemas inteligentes. Las polÃ­ticas deben abordar transparencia algorÃ­tmica, protecciÃ³n de datos personales, y auditabilidad de decisiones automatizadas.\n",
    "\n",
    "Los frameworks de governance mÃ¡s adoptados incluyen el AI Risk Management Framework de NIST, las directrices de la EU AI Act, y los principios de Responsible AI de Microsoft y Google. Estos marcos proporcionan estructura para evaluaciÃ³n de riesgos, documentaciÃ³n de modelos, y monitoreo continuo.\n",
    "\n",
    "Las mejores prÃ¡cticas incluyen establecer comitÃ©s de Ã©tica de IA, implementar sistemas de feedback de usuarios, y mantener registros detallados de decisiones del modelo. La inversiÃ³n en governance reduce riesgos legales y mejora la confianza del usuario.\"\"\",\n",
    "        \"category\": \"ai_governance\",\n",
    "        \"date\": \"2025-02-05\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"IntegraciÃ³n de OpenAI con Sistemas Empresariales\",\n",
    "        \"content\": \"\"\"La integraciÃ³n de APIs de OpenAI en sistemas empresariales requiere consideraciones especiales de seguridad, escalabilidad y costos. Las empresas implementan proxies y gateways para controlar acceso, monitorear uso, y implementar rate limiting.\n",
    "\n",
    "Los patrones arquitectÃ³nicos mÃ¡s efectivos incluyen el uso de message queues para procesar requests asincrÃ³nicamente, implementaciÃ³n de caching para reducir costos de API, y sistemas de fallback para garantizar disponibilidad.\n",
    "\n",
    "Las herramientas de observabilidad como LangSmith, Weights & Biases, y Helicone permiten monitorear rendimiento, trackear costos, y detectar anomalÃ­as en tiempo real. La implementaciÃ³n gradual con testing A/B minimiza riesgos operacionales.\"\"\",\n",
    "        \"category\": \"enterprise_integration\",\n",
    "        \"date\": \"2025-02-10\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Guardar documentos como archivos de texto individuales\n",
    "for i, doc in enumerate(synthetic_documents):\n",
    "    filename = data_dir / f\"doc_{i+1}_{doc['category']}.txt\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"TÃ­tulo: {doc['title']}\\n\")\n",
    "        f.write(f\"Fecha: {doc['date']}\\n\")\n",
    "        f.write(f\"CategorÃ­a: {doc['category']}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(doc['content'])\n",
    "\n",
    "# TambiÃ©n guardar metadata en JSON\n",
    "metadata_file = data_dir / \"documents_metadata.json\"\n",
    "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(synthetic_documents, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Generados {len(synthetic_documents)} documentos en '{data_dir}'\")\n",
    "print(f\"ğŸ“‹ CategorÃ­as: {set(doc['category'] for doc in synthetic_documents)}\")\n",
    "\n",
    "# Mostrar estadÃ­sticas\n",
    "total_words = sum(len(doc['content'].split()) for doc in synthetic_documents)\n",
    "print(f\"ğŸ“Š Total de palabras: {total_words:,}\")\n",
    "print(f\"ğŸ“ˆ Promedio por documento: {total_words // len(synthetic_documents):,} palabras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1720877666757,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "jhbYwIBMrLTJ",
    "outputId": "d3ed5f31-7f97-4a15-e2ad-3e2920c80f70"
   },
   "outputs": [],
   "source": [
    "# ğŸ“ˆ GeneraciÃ³n de Datos Estructurados\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Configurar seed para reproducibilidad\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 1. CSV con mÃ©tricas empresariales\n",
    "print(\"ğŸ“Š Generando mÃ©tricas empresariales...\")\n",
    "\n",
    "# Generar datos de ventas mensuales\n",
    "months = pd.date_range(start='2024-01-01', end='2024-12-31', freq='M')\n",
    "departments = ['Ventas', 'Marketing', 'Soporte TÃ©cnico', 'Desarrollo', 'Recursos Humanos']\n",
    "\n",
    "sales_data = []\n",
    "for month in months:\n",
    "    for dept in departments:\n",
    "        # Simular datos realistas con tendencias\n",
    "        base_revenue = random.randint(50000, 200000)\n",
    "        seasonal_factor = 1.2 if month.month in [11, 12] else 1.0  # Mayor ventas en fin de aÃ±o\n",
    "        \n",
    "        record = {\n",
    "            'fecha': month.strftime('%Y-%m-%d'),\n",
    "            'departamento': dept,\n",
    "            'ingresos': int(base_revenue * seasonal_factor * (1 + np.random.normal(0, 0.1))),\n",
    "            'gastos': int(base_revenue * 0.7 * (1 + np.random.normal(0, 0.15))),\n",
    "            'empleados': random.randint(5, 25),\n",
    "            'proyectos_activos': random.randint(2, 10),\n",
    "            'satisfaccion_cliente': round(random.uniform(3.5, 5.0), 1),\n",
    "            'horas_trabajadas': random.randint(160, 200)\n",
    "        }\n",
    "        record['beneficio'] = record['ingresos'] - record['gastos']\n",
    "        sales_data.append(record)\n",
    "\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "sales_file = data_dir / \"metricas_empresariales.csv\"\n",
    "sales_df.to_csv(sales_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"âœ… CSV generado: {sales_file}\")\n",
    "print(f\"ğŸ“‹ {len(sales_df)} registros, {len(sales_df.columns)} columnas\")\n",
    "print(f\"ğŸ¢ Departamentos: {departments}\")\n",
    "\n",
    "# Mostrar muestra de datos\n",
    "print(\"\\nğŸ“Š Muestra de datos:\")\n",
    "print(sales_df.head())\n",
    "\n",
    "# 2. Excel con informaciÃ³n de empleados\n",
    "print(\"\\nğŸ‘¥ Generando datos de empleados...\")\n",
    "\n",
    "# Listas para generar nombres realistas\n",
    "nombres = ['Ana', 'Carlos', 'MarÃ­a', 'JosÃ©', 'Laura', 'David', 'Sofia', 'Miguel', 'Carmen', 'Javier',\n",
    "          'Elena', 'Pablo', 'LucÃ­a', 'Antonio', 'Isabel', 'Francisco', 'Beatriz', 'Manuel', 'Teresa', 'Diego']\n",
    "apellidos = ['GarcÃ­a', 'RodrÃ­guez', 'GonzÃ¡lez', 'FernÃ¡ndez', 'LÃ³pez', 'MartÃ­nez', 'SÃ¡nchez', 'PÃ©rez', \n",
    "            'GÃ³mez', 'MartÃ­n', 'JimÃ©nez', 'Ruiz', 'HernÃ¡ndez', 'DÃ­az', 'Moreno', 'MuÃ±oz', 'Ãlvarez']\n",
    "\n",
    "positions = {\n",
    "    'Ventas': ['Vendedor Junior', 'Vendedor Senior', 'Gerente de Ventas', 'Director Comercial'],\n",
    "    'Marketing': ['Especialista Marketing', 'Community Manager', 'Gerente de Marketing', 'Director de Marketing'],\n",
    "    'Soporte TÃ©cnico': ['TÃ©cnico Soporte', 'Especialista Senior', 'Supervisor Soporte', 'Gerente TÃ©cnico'],\n",
    "    'Desarrollo': ['Desarrollador Junior', 'Desarrollador Senior', 'Tech Lead', 'Director de TecnologÃ­a'],\n",
    "    'Recursos Humanos': ['Asistente RRHH', 'Especialista RRHH', 'Gerente RRHH', 'Director de Personas']\n",
    "}\n",
    "\n",
    "employees_data = []\n",
    "employee_id = 1000\n",
    "\n",
    "for dept in departments:\n",
    "    num_employees = random.randint(8, 15)  # Cada departamento tiene entre 8-15 empleados\n",
    "    \n",
    "    for _ in range(num_employees):\n",
    "        name = f\"{random.choice(nombres)} {random.choice(apellidos)}\"\n",
    "        position = random.choice(positions[dept])\n",
    "        \n",
    "        # Salarios basados en posiciÃ³n y experiencia\n",
    "        salary_ranges = {\n",
    "            'Junior': (35000, 50000),\n",
    "            'Senior': (55000, 75000), \n",
    "            'Especialista': (50000, 70000),\n",
    "            'Gerente': (70000, 95000),\n",
    "            'Director': (95000, 150000),\n",
    "            'Tech Lead': (80000, 120000),\n",
    "            'Community Manager': (40000, 60000)\n",
    "        }\n",
    "        \n",
    "        # Determinar rango salarial basado en tÃ­tulo\n",
    "        salary_range = (40000, 60000)  # default\n",
    "        for key, range_val in salary_ranges.items():\n",
    "            if key in position:\n",
    "                salary_range = range_val\n",
    "                break\n",
    "                \n",
    "        employee = {\n",
    "            'id_empleado': employee_id,\n",
    "            'nombre_completo': name,\n",
    "            'departamento': dept,\n",
    "            'posicion': position,\n",
    "            'salario_anual': random.randint(salary_range[0], salary_range[1]),\n",
    "            'fecha_ingreso': (datetime.now() - timedelta(days=random.randint(30, 1800))).strftime('%Y-%m-%d'),\n",
    "            'email': f\"{name.lower().replace(' ', '.')}@empresa.com\",\n",
    "            'telefono': f\"+34 6{random.randint(10000000, 99999999)}\",\n",
    "            'nivel_experiencia': random.choice(['Junior', 'Mid', 'Senior']),\n",
    "            'habilidades': ', '.join(random.sample(['Python', 'JavaScript', 'SQL', 'Excel', 'PowerBI', \n",
    "                                                  'Marketing Digital', 'CRM', 'AnÃ¡lisis de Datos', \n",
    "                                                  'GestiÃ³n de Proyectos', 'ComunicaciÃ³n'], \n",
    "                                                random.randint(2, 5))),\n",
    "            'evaluacion_performance': round(random.uniform(3.0, 5.0), 1),\n",
    "            'dias_vacaciones_restantes': random.randint(5, 25)\n",
    "        }\n",
    "        employees_data.append(employee)\n",
    "        employee_id += 1\n",
    "\n",
    "employees_df = pd.DataFrame(employees_data)\n",
    "\n",
    "# Crear Excel con mÃºltiples hojas\n",
    "excel_file = data_dir / \"empleados_empresa.xlsx\"\n",
    "with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "    # Hoja principal con todos los empleados\n",
    "    employees_df.to_excel(writer, sheet_name='Empleados', index=False)\n",
    "    \n",
    "    # Hoja con resumen por departamento\n",
    "    dept_summary = employees_df.groupby('departamento').agg({\n",
    "        'id_empleado': 'count',\n",
    "        'salario_anual': ['mean', 'sum'],\n",
    "        'evaluacion_performance': 'mean',\n",
    "        'dias_vacaciones_restantes': 'sum'\n",
    "    }).round(2)\n",
    "    dept_summary.columns = ['Num_Empleados', 'Salario_Promedio', 'Salarios_Total', \n",
    "                           'Performance_Promedio', 'Vacaciones_Pendientes']\n",
    "    dept_summary.to_excel(writer, sheet_name='Resumen_Departamental')\n",
    "    \n",
    "    # Hoja con estadÃ­sticas de habilidades\n",
    "    all_skills = []\n",
    "    for skills_str in employees_df['habilidades']:\n",
    "        all_skills.extend([skill.strip() for skill in skills_str.split(',')])\n",
    "    \n",
    "    skills_count = pd.Series(all_skills).value_counts()\n",
    "    skills_df = pd.DataFrame({'Habilidad': skills_count.index, 'Frecuencia': skills_count.values})\n",
    "    skills_df.to_excel(writer, sheet_name='Habilidades', index=False)\n",
    "\n",
    "print(f\"âœ… Excel generado: {excel_file}\")\n",
    "print(f\"ğŸ‘¥ {len(employees_df)} empleados generados\")\n",
    "print(f\"ğŸ¢ Distribuidos en {len(departments)} departamentos\")\n",
    "\n",
    "# Mostrar estadÃ­sticas\n",
    "print(f\"\\nğŸ“Š EstadÃ­sticas de empleados:\")\n",
    "print(f\"ğŸ’° Salario promedio: ${employees_df['salario_anual'].mean():,.0f}\")\n",
    "print(f\"â­ Performance promedio: {employees_df['evaluacion_performance'].mean():.1f}/5.0\")\n",
    "print(f\"ğŸ–ï¸ Vacaciones pendientes: {employees_df['dias_vacaciones_restantes'].sum()} dÃ­as totales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U46inHYFaAE"
   },
   "source": [
    "# ğŸ—‚ï¸ RAG BÃ¡sico con Qdrant Cloud\n",
    "\n",
    "## De FAISS Local a Qdrant Cloud: La EvoluciÃ³n\n",
    "\n",
    "### **Â¿Por quÃ© migrar de FAISS a Qdrant?**\n",
    "\n",
    "| CaracterÃ­stica | FAISS (Local) | Qdrant Cloud |\n",
    "|----------------|---------------|--------------|\n",
    "| **Escalabilidad** | Limitada por RAM | Horizontal, millones de vectores |\n",
    "| **Persistencia** | Manual (save/load) | AutomÃ¡tica |\n",
    "| **BÃºsqueda HÃ­brida** | Solo densa | Densa + Sparse + Filtros |\n",
    "| **Disponibilidad** | Solo local | API REST global |\n",
    "| **ColaboraciÃ³n** | Archivos compartidos | URL compartida |\n",
    "| **Mantenimiento** | Manual | Gestionado |\n",
    "\n",
    "### **Ventajas de Qdrant en 2025:**\n",
    "\n",
    "- **ğŸš€ Performance**: BÃºsquedas sub-100ms en millones de vectores\n",
    "- **ğŸ”„ Hybrid Search**: Combina embeddings densos con bÃºsqueda lÃ©xica  \n",
    "- **ğŸ“Š Rich Filtering**: Filtros complejos por metadata\n",
    "- **âš¡ Real-time Updates**: InserciÃ³n y actualizaciÃ³n en tiempo real\n",
    "- **ğŸ›¡ï¸ Production Ready**: Backup automÃ¡tico, monitoring, security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ ConfiguraciÃ³n de Qdrant Cloud\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore  \n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import uuid\n",
    "\n",
    "# Inicializar clientes\n",
    "print(\"ğŸš€ Inicializando conexiones...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Cliente Qdrant\n",
    "    qdrant_client = QdrantClient(\n",
    "        url=os.getenv(\"QDRANT_URL\"),\n",
    "        api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    "    )\n",
    "    \n",
    "    # Test de conexiÃ³n\n",
    "    collections = qdrant_client.get_collections()\n",
    "    print(f\"âœ… Conectado a Qdrant Cloud\")\n",
    "    print(f\"ğŸ“Š Colecciones existentes: {len(collections.collections)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error conectando con Qdrant: {e}\")\n",
    "    print(\"ğŸ’¡ Verifica tus credenciales QDRANT_URL y QDRANT_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear colecciÃ³n si no existe \n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=\"rag_documents_test\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar colecciÃ³n de prueba al final\n",
    "# (Descomentar si se desea limpiar al final del notebook)\n",
    "qdrant_client.delete_collection(\"rag_documents_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings y LLM\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    dimensions=256  # DimensiÃ³n reducida para eficiencia\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"âœ… OpenAI embeddings y LLM configurados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1720877681081,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "s9O-3WlFF9dx",
    "outputId": "a135c136-155f-42f4-f369-a84b9e4b63f8"
   },
   "outputs": [],
   "source": [
    "chunk_size = 10000\n",
    "chunk_overlap = int(chunk_size * 0.25)  # 20% de overlap para evitar pÃ©rdida de contexto\n",
    "\n",
    "# ConfiguraciÃ³n para el splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(\"âœ… Text splitter configurado\")\n",
    "print(f\"ğŸ“ Chunk size: {chunk_size} chars\")\n",
    "print(f\"ğŸ”„ Overlap: {chunk_overlap} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š Carga y Procesamiento de Documentos\n",
    "print(\"ğŸ“– Cargando documentos sintÃ©ticos...\")\n",
    "\n",
    "# Cargar todos los documentos de texto\n",
    "documents = []\n",
    "doc_files = list(data_dir.glob(\"*.txt\"))\n",
    "doc_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use regex para extraer metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_file in doc_files:\n",
    "    print(f\"ğŸ“„ Procesando: {doc_file.name}\")\n",
    "    \n",
    "    with open(doc_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extraer metadata del contenido\n",
    "    # Nota: En escenarios reales, la extracciÃ³n de metadata podrÃ­a hacerse usando un LLM con structured outputs.\n",
    "    # Por ejemplo, podrÃ­amos pedirle al modelo que identifique tÃ­tulo, fecha, categorÃ­a, etc. directamente del texto.\n",
    "    # AquÃ­ lo hacemos manualmente porque nosotros construimos el documento y el formato es controlado.\n",
    "    lines = content.split('\\n')\n",
    "    title = lines[0].replace('TÃ­tulo: ', '')\n",
    "    date = lines[1].replace('Fecha: ', '')\n",
    "    category = lines[2].replace('CategorÃ­a: ', '')\n",
    "    \n",
    "    # El contenido real empieza despuÃ©s del separador\n",
    "    content_start = content.find('=' * 50) + 52\n",
    "    actual_content = content[content_start:].strip()\n",
    "    \n",
    "    # Crear documento con metadata rica\n",
    "    doc = Document(\n",
    "        page_content=actual_content,\n",
    "        metadata={\n",
    "            \"title\": title,\n",
    "            \"date\": date, \n",
    "            \"category\": category,\n",
    "            \"source\": str(doc_file),\n",
    "            \"doc_id\": doc_file.stem\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"âœ… Cargados {len(documents)} documentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtracciÃ³n de Metadata usando OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ğŸ¤– ExtracciÃ³n de Metadata con LLM\n",
    "print(\"ğŸ§  Configurando extracciÃ³n inteligente de metadata...\")\n",
    "\n",
    "# 1. Modelo Pydantic para metadata\n",
    "class DocumentMetadata(BaseModel):\n",
    "    \"\"\"Clase para la extracciÃ³n de metadata de documentos\"\"\"\n",
    "    title: str = Field(description=\"TÃ­tulo del documento que vas a reviar\")\n",
    "    date: str = Field(description=\"Fecha en formato YYYY-MM-DD, null si no hay fecha\")\n",
    "    category: str = Field(description=\"CategorÃ­a del documento, por ejemplo 'tecnologÃ­a', 'negocios', etc.\")\n",
    "    resumen: str = Field(description=\"Resumen breve del contenido del documento en mÃ¡ximo 20 palabras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prompt para extracciÃ³n de metadata\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Extrae la metadata del siguiente documento:\n",
    "\n",
    "{document_content}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Modelo de salida Pydantic\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "llm_with_structured_output = llm.with_structured_output(DocumentMetadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Cadena de extracciÃ³n\n",
    "extraction_chain = prompt | llm_with_structured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contenido_de_prueba = \"\"\"\n",
    "Date is: 2025-01-15\n",
    "IntroducciÃ³n a la Inteligencia Artificial en 2025\n",
    "==================================================\n",
    "\n",
    "La Inteligencia Artificial en 2025 ha revolucionado mÃºltiples industrias. Los modelos de lenguaje grande (LLMs) como GPT-4 y Claude han democratizado el acceso a capacidades avanzadas de procesamiento de lenguaje natural. \n",
    "\n",
    "Las principales aplicaciones incluyen asistentes virtuales inteligentes, sistemas de recomendaciÃ³n personalizados, y herramientas de automatizaciÃ³n empresarial. La integraciÃ³n de IA en procesos de negocio ha mejorado la eficiencia operativa en un 40% promedio.\n",
    "\n",
    "Los desafÃ­os actuales incluyen la gestiÃ³n de sesgos algorÃ­tmicos, la interpretabilidad de modelos complejos, y el consumo energÃ©tico de los centros de datos. Las empresas estÃ¡n adoptando enfoques de IA responsable para mitigar estos riesgos.\n",
    "\"\"\"\n",
    "\n",
    "response = extraction_chain.invoke({\"document_content\": contenido_de_prueba})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FunciÃ³n para extraer metadata\n",
    "def extract_metadata_with_llm(content: str, filename: str) -> dict:\n",
    "    result = extraction_chain.invoke({\"document_content\": content})\n",
    "    \n",
    "    return {\n",
    "        \"title\": result.title,\n",
    "        \"date\": result.date,\n",
    "        \"category\": result.category,\n",
    "        \"source\": filename\n",
    "    }\n",
    "\n",
    "# 5. Procesar documentos\n",
    "print(\"ğŸ“– Procesando documentos...\")\n",
    "\n",
    "documents = []\n",
    "for doc_file in doc_files:\n",
    "    with open(doc_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    metadata = extract_metadata_with_llm(content, doc_file.name)\n",
    "    \n",
    "    doc = Document(page_content=content, metadata=metadata)\n",
    "    documents.append(doc)\n",
    "    \n",
    "    print(f\"âœ… {metadata['title']}\")\n",
    "\n",
    "print(f\"\\nğŸ“„ {len(documents)} documentos procesados con metadata LLM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar estadÃ­sticas de documentos\n",
    "categories = [doc.metadata['category'] for doc in documents]\n",
    "category_counts = pd.Series(categories).value_counts()\n",
    "\n",
    "print(f\"\\nğŸ“Š DistribuciÃ³n por categorÃ­as:\")\n",
    "for cat, count in category_counts.items():\n",
    "    print(f\"  â€¢ {cat}: {count} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting de documentos\n",
    "print(f\"\\nâœ‚ï¸  Dividiendo documentos en chunks...\")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"âœ… Generados {len(splits)} chunks\")\n",
    "print(f\"ğŸ“Š Promedio: {len(splits)/len(documents):.1f} chunks por documento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar ejemplo de chunk\n",
    "print(f\"\\nğŸ” Ejemplo de chunk:\")\n",
    "sample_chunk = splits[5]\n",
    "print(f\"ğŸ“ Contenido: {sample_chunk.page_content[:200]}...\")\n",
    "print(f\"ğŸ·ï¸  Metadata: {sample_chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ EstadÃ­sticas de longitud de chunks\n",
    "\n",
    "# ExplicaciÃ³n educativa:\n",
    "# Analizar la longitud de los chunks es clave para entender cÃ³mo el splitting afecta la granularidad de la bÃºsqueda.\n",
    "# Chunks muy cortos pueden perder contexto, mientras que chunks muy largos pueden dificultar la recuperaciÃ³n precisa.\n",
    "\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in splits]\n",
    "word_counts = [len(chunk.page_content.split()) for chunk in splits]\n",
    "\n",
    "print(f\"\\nğŸ“ EstadÃ­sticas de chunks:\")\n",
    "print(f\"  â€¢ Longitud promedio: {sum(chunk_lengths)/len(chunk_lengths):.0f} caracteres\")\n",
    "print(f\"  â€¢ Chunk mÃ¡s corto: {min(chunk_lengths)} caracteres\") \n",
    "print(f\"  â€¢ Chunk mÃ¡s largo: {max(chunk_lengths)} caracteres\")\n",
    "print(f\"  â€¢ Promedio de palabras por chunk: {sum(word_counts)/len(word_counts):.0f}\")\n",
    "print(f\"  â€¢ Chunk con menos palabras: {min(word_counts)}\")\n",
    "print(f\"  â€¢ Chunk con mÃ¡s palabras: {max(word_counts)}\")\n",
    "print(f\"  â€¢ Total de chunks: {len(splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ CreaciÃ³n del Vector Store en Qdrant Cloud\n",
    "\n",
    "En esta secciÃ³n vamos a construir nuestro **Vector Store** utilizando Qdrant Cloud. Este paso es fundamental para habilitar bÃºsquedas semÃ¡nticas eficientes sobre nuestros documentos sintÃ©ticos. AprenderÃ¡s cÃ³mo indexar los chunks generados y realizar bÃºsquedas vectoriales que serÃ¡n la base de nuestro sistema RAG moderno.\n",
    "\n",
    "**Â¿QuÃ© haremos?**\n",
    "- Crear una colecciÃ³n en Qdrant Cloud\n",
    "- Indexar los documentos procesados y chunkificados\n",
    "- Realizar una bÃºsqueda de prueba para validar la indexaciÃ³n\n",
    "\n",
    "> ğŸ’¡ *Recuerda: El Vector Store es el corazÃ³n de cualquier sistema RAG, permitiendo recuperar informaciÃ³n relevante de manera rÃ¡pida y escalable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ CreaciÃ³n del Vector Store en Qdrant Cloud\n",
    "import time\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "# Nombre de la colecciÃ³n\n",
    "collection_name = \"rag_avanzado_2025\"\n",
    "\n",
    "print(f\"ğŸ—ƒï¸  Creando colecciÃ³n '{collection_name}' en Qdrant Cloud...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Verificar si la colecciÃ³n ya existe\n",
    "    existing_collections = [col.name for col in qdrant_client.get_collections().collections]\n",
    "    \n",
    "    if collection_name in existing_collections:\n",
    "        print(f\"âš ï¸  La colecciÃ³n '{collection_name}' ya existe. EliminÃ¡ndola...\")\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "        time.sleep(2)  # Esperar a que se complete la eliminaciÃ³n\n",
    "    \n",
    "    # Crear el vector store usando LangChain + Qdrant\n",
    "    print(\"ğŸ”„ Creando vector store e indexando documentos...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    vector_store = QdrantVectorStore.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        url=os.getenv(\"QDRANT_URL\"),\n",
    "        api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    "        collection_name=collection_name,\n",
    "        force_recreate=True,  # Recrear si existe\n",
    "        batch_size = 1\n",
    "    )\n",
    "    \n",
    "    index_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"âœ… Vector store creado exitosamente!\")\n",
    "    print(f\"â±ï¸  Tiempo de indexado: {index_time:.2f} segundos\")\n",
    "    print(f\"ğŸ“Š {len(splits)} chunks indexados\")\n",
    "    \n",
    "    # Verificar la colecciÃ³n\n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    print(f\"ğŸ” Vectores en la colecciÃ³n: {collection_info.vectors_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creando vector store: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bÃ¡sico de bÃºsqueda\n",
    "print(f\"\\nğŸ” Probando bÃºsqueda bÃ¡sica...\")\n",
    "\n",
    "query = \"Â¿QuÃ© ventajas tiene Qdrant como base de datos vectorial?\"\n",
    "print(f\"â“ Consulta: {query}\")\n",
    "\n",
    "# BÃºsqueda simple\n",
    "search_results = vector_store.similarity_search(query, k=5)\n",
    "\n",
    "print(f\"\\nğŸ“„ Resultados encontrados:\")\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"\\n--- Resultado {i} ---\")\n",
    "    print(f\"ğŸ“š TÃ­tulo: {result.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  CategorÃ­a: {result.metadata.get('category', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Contenido: {result.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1720877690136,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "7iSSntYqGhHk",
    "outputId": "7b3b90fe-3035-468e-c58c-ea6b761a3390"
   },
   "outputs": [],
   "source": [
    "# BÃºsqueda con scores\n",
    "print(f\"\\nğŸ“Š BÃºsqueda con scores de similitud:\")\n",
    "search_with_scores = vector_store.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for i, (doc, score) in enumerate(search_with_scores, 1):\n",
    "    print(f\"\\nğŸ¯ Resultado {i} (Score: {score:.4f})\")\n",
    "    print(f\"ğŸ“š {doc.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"ğŸ“ {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1720877718453,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "r2XQkbj8LNbQ",
    "outputId": "87dd8486-1bb6-43a7-ee86-61cbd9bf0d73"
   },
   "source": [
    "## ğŸ”— ImplementaciÃ³n de RAG con LangChain\n",
    "\n",
    "Ahora vamos a implementar un sistema RAG completo usando LangChain Expression Language (LCEL). Esto nos darÃ¡ una base sÃ³lida antes de migrar a LangGraph.\n",
    "\n",
    "### **Componentes del Pipeline RAG:**\n",
    "\n",
    "1. **ğŸ” Retriever**: Busca documentos relevantes en Qdrant\n",
    "2. **ğŸ“ Prompt Template**: Estructura la consulta con contexto\n",
    "3. **ğŸ¤– LLM**: Genera respuesta basada en documentos recuperados  \n",
    "4. **ğŸ”„ Chain**: Conecta todos los componentes\n",
    "\n",
    "### **Mejoras vs. ImplementaciÃ³n Anterior:**\n",
    "\n",
    "- âœ… **Vector Database en la nube** (Qdrant vs FAISS local)\n",
    "- âœ… **BÃºsqueda con filtros** por metadata  \n",
    "- âœ… **Prompt engineering** mejorado\n",
    "- âœ… **GestiÃ³n de errores** robusta\n",
    "- âœ… **MÃ©tricas de performance** integradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”— CreaciÃ³n del Pipeline RAG con LCEL\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configurar el Retriever\n",
    "print(\"ğŸ” Configurando retriever...\")\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,  # Top 5 documentos mÃ¡s relevantes\n",
    "        \"score_threshold\": 0.1  # Solo documentos con alta similitud\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. FunciÃ³n para formatear documentos\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Formatea los documentos recuperados para el prompt\n",
    "    \"\"\"\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        doc_text = f\"[Documento {i}]\\n\"\n",
    "        doc_text += f\"TÃ­tulo: {doc.metadata.get('title', 'N/A')}\\n\"\n",
    "        doc_text += f\"CategorÃ­a: {doc.metadata.get('category', 'N/A')}\\n\"\n",
    "        doc_text += f\"Contenido: {doc.page_content}\\n\"\n",
    "        formatted_docs.append(doc_text)\n",
    "    \n",
    "    return \"\\n\" + \"=\"*50 + \"\\n\".join(formatted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prompt Template Avanzado\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "## ROL\n",
    "Eres un asistente experto en tecnologÃ­a e inteligencia artificial.\n",
    "\n",
    "## TAREA\n",
    "Tu tarea es responder preguntas basÃ¡ndote ÃšNICAMENTE en la informaciÃ³n proporcionada en los documentos.\n",
    "\n",
    "## INSTRUCCIONES:\n",
    "1. **Analiza cuidadosamente** todos los documentos proporcionados\n",
    "2. **Responde SOLO** con informaciÃ³n que estÃ© explÃ­citamente en los documentos  \n",
    "3. **Cita las fuentes** mencionando tÃ­tulos de documentos relevantes\n",
    "4. **Si no encuentras informaciÃ³n suficiente**, indica claramente quÃ© falta\n",
    "5. **Estructura tu respuesta** de manera clara y profesional\n",
    "\n",
    "## FORMATO DE RESPUESTA:\n",
    "- Usa pÃ¡rrafos cortos y claros\n",
    "- Incluye ejemplos si es relevante\n",
    "- No uses jerga tÃ©cnica innecesaria\n",
    "\n",
    "## CONTEXTO RECUPERADO:\n",
    "{context}\n",
    "\n",
    "## PREGUNTA DEL USUARIO:\n",
    "{question}\n",
    "\n",
    "## RESPUESTA:\n",
    "BasÃ¡ndome en los documentos proporcionados:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Construir la Chain con LCEL\n",
    "print(\"ğŸ”— Construyendo cadena RAG...\")\n",
    "\n",
    "# Chain principal que combina contexto y pregunta (LCEL)\n",
    "rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Â¿CuÃ¡les son las ventajas de usar Qdrant como base de datos vectorial?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 13159,
     "status": "ok",
     "timestamp": 1720877737334,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "c5Nb4usZKsMk",
    "outputId": "41826740-e7af-40eb-be52-ec247ae46d4c"
   },
   "outputs": [],
   "source": [
    "print(\"âœ… Cadena RAG configurada exitosamente\")\n",
    "\n",
    "# 5. FunciÃ³n de testing con mÃ©tricas\n",
    "def test_rag_query(question, show_context=False):\n",
    "    \"\"\"\n",
    "    Prueba el sistema RAG con una pregunta y muestra mÃ©tricas\n",
    "    \"\"\"\n",
    "    print(f\"â“ Pregunta: {question}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Medir tiempo y tokens\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        # Obtener contexto recuperado (para debugging)\n",
    "        if show_context:\n",
    "            retrieved_docs = retriever.invoke(question)\n",
    "            print(\"ğŸ“š DOCUMENTOS RECUPERADOS:\")\n",
    "            for i, doc in enumerate(retrieved_docs, 1):\n",
    "                print(f\"\\n[Doc {i}] {doc.metadata.get('title', 'N/A')}\")\n",
    "                print(f\"Score: N/A | CategorÃ­a: {doc.metadata.get('category', 'N/A')}\")\n",
    "                print(f\"Contenido: {doc.page_content[:200]}...\")\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "        # Generar respuesta\n",
    "        response = rag_chain.invoke(question)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Mostrar respuesta\n",
    "        print(\"ğŸ¤– RESPUESTA:\")\n",
    "        print(response)\n",
    "        \n",
    "        # Mostrar mÃ©tricas\n",
    "        print(f\"\\nğŸ“Š MÃ‰TRICAS:\")\n",
    "        print(f\"â±ï¸  Tiempo total: {end_time - start_time:.2f} segundos\")\n",
    "        print(f\"ğŸ« Tokens usados: {cb.total_tokens}\")\n",
    "        print(f\"ğŸ’° Costo aproximado: ${cb.total_cost:.4f}\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Test inicial\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ§ª PRUEBA INICIAL DEL SISTEMA RAG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "response = test_rag_query(\n",
    "    \"Â¿Que dijo el presidente Biden en su discurso?\",\n",
    "    show_context=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1720878957909,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "0_X8pr6gIgRp",
    "outputId": "d9591c6b-79c6-425a-843d-9bd7edea9d1c"
   },
   "outputs": [],
   "source": [
    "# ğŸ¯ Pruebas Finales del RAG BÃ¡sico\n",
    "\n",
    "print(\"ğŸ§ª BATERÃA DE PRUEBAS RAG BÃSICO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Conjunto de preguntas de prueba diversas\n",
    "test_questions = [\n",
    "    \"Â¿QuÃ© es RAG y cuÃ¡les son sus tÃ©cnicas avanzadas mencionadas?\",\n",
    "    \"Â¿CuÃ¡les son las diferencias entre Qdrant, Pinecone y Weaviate?\", \n",
    "    \"Â¿QuÃ© ventajas ofrece LangGraph para sistemas agentic?\",\n",
    "    \"Â¿CuÃ¡les son los desafÃ­os principales de la IA en 2025?\",\n",
    "    \"Â¿QuÃ© consideraciones de governance debe tener una empresa que implementa IA?\",\n",
    "    \"Â¿CÃ³mo se integra OpenAI con sistemas empresariales existentes?\",\n",
    "    \"Â¿Que dijo el presidente Biden en su discurso sobre el Covid?\",\n",
    "]\n",
    "\n",
    "# Ejecutar todas las pruebas\n",
    "results = []\n",
    "total_time = 0\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nğŸ“ PRUEBA {i}/{len(test_questions)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = test_rag_query(question, show_context=False)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        total_time += execution_time\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'response': response,\n",
    "            'time': execution_time,\n",
    "            'status': 'success'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'response': None,\n",
    "            'time': 0,\n",
    "            'status': 'error',\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "# Resumen de resultados\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š RESUMEN DE RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "successful_tests = [r for r in results if r['status'] == 'success']\n",
    "failed_tests = [r for r in results if r['status'] == 'error']\n",
    "\n",
    "print(f\"âœ… Pruebas exitosas: {len(successful_tests)}/{len(test_questions)}\")\n",
    "print(f\"âŒ Pruebas fallidas: {len(failed_tests)}/{len(test_questions)}\")\n",
    "print(f\"â±ï¸  Tiempo total: {total_time:.2f} segundos\")\n",
    "print(f\"ğŸ“ˆ Tiempo promedio por consulta: {total_time/len(successful_tests):.2f} segundos\")\n",
    "\n",
    "if failed_tests:\n",
    "    print(f\"\\nğŸ” Errores encontrados:\")\n",
    "    for test in failed_tests:\n",
    "        print(f\"  â€¢ {test['question'][:50]}... - {test['error']}\")\n",
    "\n",
    "# AnÃ¡lisis de calidad de respuestas\n",
    "print(f\"\\nğŸ“ˆ ANÃLISIS DE CALIDAD\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "quality_metrics = {\n",
    "    'responses_with_sources': 0,\n",
    "    'responses_with_categories': 0,\n",
    "    'comprehensive_responses': 0\n",
    "}\n",
    "\n",
    "for result in successful_tests:\n",
    "    response = result['response'].lower()\n",
    "    \n",
    "    # Verificar si menciona fuentes/tÃ­tulos\n",
    "    if any(word in response for word in ['tÃ­tulo', 'documento', 'segÃºn', 'basÃ¡ndome']):\n",
    "        quality_metrics['responses_with_sources'] += 1\n",
    "    \n",
    "    # Verificar si menciona categorÃ­as especÃ­ficas  \n",
    "    if any(cat in response for cat in ['rag', 'langgraph', 'qdrant', 'openai', 'governance']):\n",
    "        quality_metrics['responses_with_categories'] += 1\n",
    "        \n",
    "    # Verificar respuestas comprehensivas (mÃ¡s de 100 palabras)\n",
    "    if len(response.split()) > 100:\n",
    "        quality_metrics['comprehensive_responses'] += 1\n",
    "\n",
    "print(f\"ğŸ“š Respuestas que citan fuentes: {quality_metrics['responses_with_sources']}/{len(successful_tests)}\")\n",
    "print(f\"ğŸ·ï¸  Respuestas especÃ­ficas por categorÃ­a: {quality_metrics['responses_with_categories']}/{len(successful_tests)}\")\n",
    "print(f\"ğŸ“ Respuestas comprehensivas: {quality_metrics['comprehensive_responses']}/{len(successful_tests)}\")\n",
    "\n",
    "print(f\"\\nâœ… RAG bÃ¡sico con Qdrant implementado y probado exitosamente\")\n",
    "print(f\"ğŸš€ Listo para migrar a LangGraph para capacidades agentic avanzadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8wiBD-XQV1y"
   },
   "source": [
    "# Otras Tecnicas Avanzadas para mejorar un RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIHEUzZYQedT"
   },
   "source": [
    "## âœï¸ Rewrite-Retrieve-Read: TÃ©cnica de Mejora de Consultas en RAG\n",
    "\n",
    "La tÃ©cnica **Rewrite-Retrieve-Read** parte de la idea de que reescribir la consulta del usuario puede mejorar significativamente la recuperaciÃ³n de informaciÃ³n en sistemas RAG, especialmente cuando usamos modelos de lenguaje grande (LLMs). Al optimizar los tÃ©rminos de bÃºsqueda, logramos una mayor alineaciÃ³n semÃ¡ntica entre la consulta y los documentos, lo que se traduce en resultados mÃ¡s relevantes y precisos.\n",
    "\n",
    "### ğŸš¦ Proceso Paso a Paso\n",
    "\n",
    "1. **Reescritura de la consulta**  \n",
    "    El LLM toma la consulta original y la transforma, enfocÃ¡ndose en mejorar los tÃ©rminos clave para la bÃºsqueda.\n",
    "\n",
    "2. **RecuperaciÃ³n con la nueva consulta**  \n",
    "    Se utiliza la consulta reescrita para buscar informaciÃ³n en la base de datos o vector store, maximizando la relevancia de los resultados.\n",
    "\n",
    "3. **GeneraciÃ³n de la respuesta**  \n",
    "    El sistema genera la respuesta final combinando el contexto recuperado y la consulta original, asegurando precisiÃ³n y cobertura.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Ventajas Educativas\n",
    "\n",
    "- **Mejor alineaciÃ³n semÃ¡ntica**: Reduce la brecha entre lo que el usuario pregunta y cÃ³mo estÃ¡ indexada la informaciÃ³n.\n",
    "- **Resultados mÃ¡s precisos**: Facilita la recuperaciÃ³n de documentos realmente relevantes.\n",
    "- **Aplicable a cualquier dominio**: Ãštil tanto en preguntas tÃ©cnicas como generales.\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ’¡ *Esta tÃ©cnica es especialmente poderosa en escenarios donde las consultas de los usuarios son ambiguas, poco especÃ­ficas o contienen tÃ©rminos poco frecuentes. Reescribir la consulta ayuda al sistema a â€œpensar como el buscadorâ€, mejorando la experiencia y la calidad de las respuestas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMVvi50RQdqU"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1720880237048,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "4pWfmOOFQZkS",
    "outputId": "7dc2fcc3-df2e-4c23-ef1d-806dd207c581"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vector_store.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"â€œRewrite the userâ€™s query focusing on improving the search terms, make it vervose, mantain the original language.\n",
    "\n",
    "Original query: {query}\"\"\")\n",
    "\n",
    "improve_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "improve_chain.invoke(\"Como siento ho?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 855,
     "status": "ok",
     "timestamp": 1720880636927,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "qdRIQ0OoTw-Z",
    "outputId": "341b9640-6fe9-465f-fdbb-11130eff1c2b"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ImplementaciÃ³n simple.\n",
    "pregunta_original = \"president said of covid-19\"\n",
    "\n",
    "start_time = time.time()\n",
    "pregunta_mejorada = improve_chain.invoke(pregunta_original)\n",
    "improve_time = time.time()\n",
    "\n",
    "print(f'Tiempo para mejorar la pregunta: {improve_time - start_time} segundos')\n",
    "pregunta_mejorada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 1845,
     "status": "ok",
     "timestamp": 1720880653182,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "IDo2JpUJDcYx",
    "outputId": "566a24dd-6a68-440c-e13f-cab7ed317447"
   },
   "outputs": [],
   "source": [
    "# Ahora usamos nuestro rag chain anterior usando el documento hipotÃ©tico como RAG\n",
    "start_time = time.time()\n",
    "respuesta = rag_chain.invoke(pregunta_mejorada)\n",
    "response_time = time.time()\n",
    "\n",
    "print(f'Tiempo para obtener la respuesta: {response_time - start_time} segundos')\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readrewrite(question):\n",
    "    pregunta_mejorada = improve_chain.invoke(question)\n",
    "    respuesta = rag_chain.invoke(pregunta_mejorada)\n",
    "    return respuesta\n",
    "\n",
    "readrewrite(pregunta_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAHqTzp_RIV3"
   },
   "source": [
    "## Embeddings de Documentos HipotÃ©ticos (HyDE)\n",
    "\n",
    "### ğŸ§  Â¿QuÃ© es HyDE?\n",
    "\n",
    "**Hypothetical Document Embeddings (HyDE)** es una tÃ©cnica avanzada para mejorar la recuperaciÃ³n de informaciÃ³n en sistemas RAG. En vez de buscar directamente con la consulta del usuario, HyDE utiliza un modelo de lenguaje (LLM) para generar un documento hipotÃ©tico que podrÃ­a responder la pregunta. Este documento se convierte en embedding y se usa como vector de bÃºsqueda, logrando una mayor alineaciÃ³n semÃ¡ntica con los documentos relevantes.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Â¿Por quÃ© usar HyDE?\n",
    "\n",
    "- **AlineaciÃ³n semÃ¡ntica mejorada**: El documento generado por el LLM utiliza el mismo lenguaje y contexto que los documentos reales, facilitando la recuperaciÃ³n de informaciÃ³n relevante.\n",
    "- **ReducciÃ³n de la brecha lÃ©xica**: Si la consulta del usuario usa tÃ©rminos diferentes a los del corpus, HyDE traduce la intenciÃ³n al vocabulario adecuado.\n",
    "- **Resultados mÃ¡s precisos**: Al buscar con un embedding generado por el LLM, se incrementa la probabilidad de encontrar respuestas Ãºtiles y contextuales.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Ejemplo de Flujo HyDE\n",
    "\n",
    "1. **Usuario pregunta**: â€œÂ¿QuÃ© dijo el presidente sobre Ketanji Brown Jackson?â€\n",
    "2. **LLM genera documento hipotÃ©tico**: Un pÃ¡rrafo que podrÃ­a responder la pregunta.\n",
    "3. **Embedding del documento hipotÃ©tico**: Se convierte en vector y se usa para buscar en la base de datos.\n",
    "4. **RecuperaciÃ³n y respuesta**: Se obtienen los documentos mÃ¡s relevantes y se genera la respuesta final.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Ventajas Educativas\n",
    "\n",
    "- Permite experimentar con prompts y generaciÃ³n de contexto.\n",
    "- Demuestra cÃ³mo los LLM pueden mejorar la bÃºsqueda mÃ¡s allÃ¡ de la consulta original.\n",
    "- Facilita la comprensiÃ³n de tÃ©cnicas modernas de RAG y retrieval semÃ¡ntico.\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ’¡ **HyDE es especialmente Ãºtil cuando las preguntas del usuario son ambiguas, poco especÃ­ficas o usan lenguaje diferente al corpus. Es una tÃ©cnica clave para sistemas RAG de Ãºltima generaciÃ³n.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_2MiMi8V3rL"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Inicializar LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1720881200261,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "vwfqosz8FGzF",
    "outputId": "8eebadae-a8bb-4231-f179-075d02aee53a"
   },
   "outputs": [],
   "source": [
    "# Plantilla del sistema para HyDE\n",
    "SYSTEM_TEMPLATE = \"\"\"Generate a hypothetical parragraph that can answer the user's query.\n",
    "\n",
    "Original query: {query}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt template\n",
    "prompt = ChatPromptTemplate.from_template(SYSTEM_TEMPLATE)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "executionInfo": {
     "elapsed": 2658,
     "status": "ok",
     "timestamp": 1720881230508,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "8VLbemkBFZVC",
    "outputId": "10bd3365-a570-4268-ab1d-5e0dd9ffc411"
   },
   "outputs": [],
   "source": [
    "# LCEL - Hype Pipeline - With Outoput Parser\n",
    "hyde_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "hyde_chain.invoke('Porque Qdrant es una buena BBDD Vecorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "executionInfo": {
     "elapsed": 2360,
     "status": "ok",
     "timestamp": 1720881272540,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "g6YpP-jAV3ng",
    "outputId": "921e2a24-f40a-4398-d3c1-d24ba4e7c20e"
   },
   "outputs": [],
   "source": [
    "# Pregunta\n",
    "question = \"Porque Qdrant es una buena BBDD Vecorial\"\n",
    "\n",
    "# Generar documento hipotÃ©tico\n",
    "start_time = time.time()\n",
    "hypothetical_document = hyde_chain.invoke(question)\n",
    "hyde_time = time.time()\n",
    "\n",
    "print(f'Tiempo para generar el documento hipotÃ©tico: {hyde_time - start_time} segundos')\n",
    "hypothetical_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "executionInfo": {
     "elapsed": 2545,
     "status": "ok",
     "timestamp": 1720881337350,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "Z5-Kaw0-C5T3",
    "outputId": "abb691ab-f05f-49b8-e444-af4adf7a64eb"
   },
   "outputs": [],
   "source": [
    "# Cantidad\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "# Instructions\n",
    "- Answer in spanish\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Recuperar documentos usando el documento hipotÃ©tico\n",
    "start_time = time.time()\n",
    "response = rag_chain.invoke(hypothetical_document)\n",
    "response_time = time.time()\n",
    "\n",
    "print(f'Tiempo para generar la respuesta: {response_time - start_time} segundos')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbDrpFw1Avc6"
   },
   "source": [
    "### MultiQueryRetriever Concepto\n",
    "\n",
    "El MultiQueryRetriever es una tÃ©cnica avanzada que mejora la recuperaciÃ³n de informaciÃ³n dividiendo una consulta compleja en mÃºltiples subconsultas mÃ¡s simples y manejables. Cada subconsulta se enfoca en un aspecto especÃ­fico de la informaciÃ³n buscada, lo que aumenta las posibilidades de recuperar documentos relevantes y precisos. Esta tÃ©cnica es especialmente Ãºtil cuando la consulta original del usuario es amplia o ambigua.\n",
    "\n",
    "Proceso\n",
    "\n",
    "1.\tDescomposiciÃ³n de la consulta: La consulta original se divide en varias subconsultas mÃ¡s pequeÃ±as.\n",
    "2.\tRecuperaciÃ³n de informaciÃ³n: Cada subconsulta se utiliza para recuperar informaciÃ³n de la base de datos.\n",
    "3.\tCombinaciÃ³n de resultados: Los resultados de todas las subconsultas se combinan para formar la respuesta final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3G_5ftYKBDdc"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJaej2MdBQin"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Creamos un modelo Pydantic para una lista de preguntas\n",
    "class MultiQueryOutput(BaseModel):\n",
    "    \"\"\"\n",
    "    Modelo para almacenar una lista de subconsultas generadas por el LLM.\n",
    "    \"\"\"\n",
    "    questions: List[str] = Field(\n",
    "        description=\"Lista de subconsultas generadas para mejorar la bÃºsqueda\"\n",
    "    )\n",
    "\n",
    "# Plantilla del sistema para dividir la consulta\n",
    "SYSTEM_TEMPLATE = \"\"\"Split the user's query into multiple smaller subqueries to improve the search.\n",
    "\n",
    "# Instructions:\n",
    "- DonÂ´t add numbers\n",
    "- Provide 5 different questions\n",
    "\n",
    "Original query: {query}\n",
    "\n",
    "Your response:\"\"\"\n",
    "\n",
    "# Our prompt\n",
    "prompt = ChatPromptTemplate.from_template(SYSTEM_TEMPLATE)\n",
    "\n",
    "llm_with_structured_output = llm.with_structured_output(MultiQueryOutput)\n",
    "\n",
    "# The chain\n",
    "mq_chain = prompt | llm_with_structured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 2268,
     "status": "ok",
     "timestamp": 1720881925134,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "6-zqFnlDH08U",
    "outputId": "4e52cf1b-f64c-4113-e1d0-54ca22d47c92"
   },
   "outputs": [],
   "source": [
    "# Question\n",
    "question = 'Cual es el modelo estado del arte de LLM? y quien compite co GPT-4'\n",
    "\n",
    "response = mq_chain.invoke(question)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar las preguntas generadas\n",
    "      \n",
    "response.model_dump()['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1378,
     "status": "ok",
     "timestamp": 1720881986069,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "CrBTlTm4Iy-6",
    "outputId": "4f5bc1d7-a933-4c66-9726-d27fe56f4523"
   },
   "outputs": [],
   "source": [
    "# Pregunta\n",
    "question = \"What did the president say about ketanji brown jackson?\"\n",
    "\n",
    "# Dividir la consulta en subconsultas\n",
    "start_time = time.time()\n",
    "subqueries = mq_chain.invoke(question)\n",
    "subqueries = subqueries.model_dump()['questions']\n",
    "mq_time = time.time()\n",
    "\n",
    "print(f'Tiempo para dividir la consulta: {mq_time - start_time} segundos')\n",
    "subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1044,
     "status": "ok",
     "timestamp": 1720882020172,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "aiOGzhCtBU5p",
    "outputId": "8d92d2d5-03c4-4083-ed3f-d44de446e2db"
   },
   "outputs": [],
   "source": [
    "# Cantidad\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Recuperar documentos usando cada subconsulta\n",
    "all_docs = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for subquery in subqueries:\n",
    "    print(f'Recuperando documentos para la subconsulta: {subquery}')\n",
    "    docs = retriever.invoke(subquery)\n",
    "\n",
    "    # Add to the list\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "retrieve_time = time.time()\n",
    "\n",
    "print(f'Tiempo para recuperar documentos: {retrieve_time - start_time} segundos')\n",
    "print(f'Q de Documentos {len(all_docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1720882084286,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "eqqjbW8YKhNl",
    "outputId": "89809941-4fad-426e-a4db-e56a62069a5a"
   },
   "outputs": [],
   "source": [
    "# Formateamos los documentos para agregarlos al contexto\n",
    "formated_docs = \"\\n\\n\".join(doc.page_content for doc in all_docs)\n",
    "formated_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 1697,
     "status": "ok",
     "timestamp": 1720882111565,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "MagHfISYJzn1",
    "outputId": "377d0953-d82a-4e20-fdc9-aa30b4bac330"
   },
   "outputs": [],
   "source": [
    "# Pregunta\n",
    "question = \"What did the president say about ketanji brown jackson?\"\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "# Instructions\n",
    "- Answer in spanish\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "# Hacemos el RAG\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Recuperar documentos usando el documento hipotÃ©tico\n",
    "start_time = time.time()\n",
    "response = rag_chain.invoke({'question': question, 'context': formated_docs})\n",
    "response_time = time.time()\n",
    "\n",
    "print(f'Tiempo para generar la respuesta: {response_time - start_time} segundos')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "uejecutivos-diplomado-rag-2024-s1-ip1ydhJZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
