{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ll2BlmZaU1Wu",
   "metadata": {
    "id": "ll2BlmZaU1Wu"
   },
   "source": [
    "# Chunking Avanzado para RAG - 2025\n",
    "\n",
    "## IntroducciÃ³n\n",
    "\n",
    "En el mundo de la Inteligencia Artificial de 2025, el **chunking** (divisiÃ³n de texto) se ha convertido en uno de los componentes mÃ¡s crÃ­ticos para el Ã©xito de sistemas RAG (Retrieval-Augmented Generation). Con la evoluciÃ³n de los modelos de lenguaje y embeddings, las estrategias de chunking han evolucionado significativamente.\n",
    "\n",
    "En este mÃ³dulo, exploraremos las **mejores prÃ¡cticas de chunking para 2025**, utilizando el stack tecnolÃ³gico mÃ¡s avanzado: **LangChain, LangGraph, Qdrant y OpenAI**. Aprenderemos cÃ³mo aplicar estas tecnologÃ­as para crear sistemas RAG de producciÃ³n que maximizan la calidad de recuperaciÃ³n y generaciÃ³n.\n",
    "\n",
    "## Â¿Por quÃ© es tan Importante el Chunking en 2025?\n",
    "\n",
    "El chunking no es simplemente dividir texto - es un arte que impacta directamente en:\n",
    "\n",
    "- **Calidad de RecuperaciÃ³n**: Chunks mal creados = respuestas irrelevantes\n",
    "- **Eficiencia Computacional**: OptimizaciÃ³n de tokens y costos de embeddings\n",
    "- **Coherencia SemÃ¡ntica**: Preservar el significado y contexto\n",
    "- **Rendimiento del Sistema**: Velocidad de bÃºsqueda y precisiÃ³n\n",
    "\n",
    "## Stack TecnolÃ³gico 2025\n",
    "\n",
    "- **ğŸ¦œ LangChain**: Framework principal para procesamiento de documentos\n",
    "- **ğŸ•¸ï¸ LangGraph**: Workflows inteligentes y RAG adaptativo  \n",
    "- **ğŸ” Qdrant**: Vector database de alta performance\n",
    "- **ğŸ¤– OpenAI**: Modelos de embeddings estado del arte (text-embedding-3-large/small)\n",
    "\n",
    "Veremos cÃ³mo integrar todas estas tecnologÃ­as para crear sistemas RAG robustos y escalables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AxwM_JnEVSMN",
   "metadata": {
    "id": "AxwM_JnEVSMN"
   },
   "source": [
    "## Estrategias de Chunking en 2025\n",
    "\n",
    "### 1. **Character Splitting** \n",
    "**DescripciÃ³n:** DivisiÃ³n bÃ¡sica por nÃºmero fijo de caracteres o tokens.\n",
    "- **âœ… Ventajas:** Simple de implementar, rÃ¡pido\n",
    "- **âŒ Desventajas:** Ignora estructura y contexto del texto\n",
    "- **ğŸ“Š Uso en 2025:** Solo para casos bÃ¡sicos o testing inicial\n",
    "\n",
    "### 2. **Recursive Character Text Splitting**\n",
    "**DescripciÃ³n:** DivisiÃ³n jerÃ¡rquica usando lista de separadores (pÃ¡rrafos â†’ lÃ­neas â†’ oraciones â†’ palabras).\n",
    "- **âœ… Ventajas:** Preserva estructura lÃ³gica, configurable\n",
    "- **âŒ Desventajas:** Puede ser computacionalmente costoso\n",
    "- **ğŸ“Š Uso en 2025:** EstÃ¡ndar para la mayorÃ­a de documentos de texto\n",
    "\n",
    "### 3. **Document-Specific Splitting**\n",
    "**DescripciÃ³n:** Estrategias especializadas por formato (Markdown, Python, HTML, JSON, etc.).\n",
    "- **âœ… Ventajas:** Mantiene integridad del formato y contexto especÃ­fico\n",
    "- **âŒ Desventajas:** Requiere parsers especializados\n",
    "- **ğŸ“Š Uso en 2025:** Esencial para documentos tÃ©cnicos y cÃ³digo\n",
    "\n",
    "### 4. **Semantic Splitting** â­ STAR DE 2025\n",
    "**DescripciÃ³n:** DivisiÃ³n basada en similitud semÃ¡ntica usando embeddings.\n",
    "- **âœ… Ventajas:** Preserva coherencia semÃ¡ntica, chunks mÃ¡s inteligentes\n",
    "- **âŒ Desventajas:** Mayor costo computacional y de embeddings\n",
    "- **ğŸ“Š Uso en 2025:** Gold standard para aplicaciones de producciÃ³n\n",
    "\n",
    "### 5. **Adaptive/Agentic Chunking** ğŸ†• NUEVO EN 2025\n",
    "**DescripciÃ³n:** Chunking dinÃ¡mico que se adapta al contenido usando LLMs.\n",
    "- **âœ… Ventajas:** OptimizaciÃ³n inteligente por documento\n",
    "- **âŒ Desventajas:** Alto costo, requiere LLM adicional\n",
    "- **ğŸ“Š Uso en 2025:** Para casos de uso premium\n",
    "\n",
    "## Recursos Actualizados 2025\n",
    "\n",
    "- **[ChunkViz.com](https://www.chunkviz.com/)** - VisualizaciÃ³n de mÃ©todos de chunking\n",
    "- **[LangChain Text Splitters](https://python.langchain.com/docs/concepts/text_splitters/)** - DocumentaciÃ³n oficial\n",
    "- **[Qdrant Chunking Guide](https://qdrant.tech/documentation/frameworks/langchain/)** - GuÃ­a de integraciÃ³n\n",
    "- **[OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)** - Modelos 2025\n",
    "\n",
    "ğŸ’¡ **Regla de Oro 2025**: Si un chunk tiene sentido para un humano sin contexto adicional, tendrÃ¡ sentido para el modelo tambiÃ©n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55VCnbK-V-pR",
   "metadata": {
    "id": "55VCnbK-V-pR"
   },
   "source": [
    "## EvaluaciÃ³n de Estrategias de Chunking en 2025\n",
    "\n",
    "La evaluaciÃ³n rigurosa de chunking es **crÃ­tica** para sistemas RAG de producciÃ³n. No existe una estrategia Ãºnica que funcione para todos los casos.\n",
    "\n",
    "### ğŸ¯ MÃ©tricas Clave para Evaluar Chunking\n",
    "\n",
    "**MÃ©tricas de Retrieval:**\n",
    "- **Precision@K**: Â¿Los chunks recuperados son relevantes?\n",
    "- **Recall@K**: Â¿Se recuperan todos los chunks relevantes?\n",
    "- **MRR (Mean Reciprocal Rank)**: Â¿Los chunks mÃ¡s relevantes aparecen primero?\n",
    "\n",
    "**MÃ©tricas de GeneraciÃ³n:**\n",
    "- **Faithfulness**: Â¿La respuesta se basa en los chunks?\n",
    "- **Answer Relevancy**: Â¿La respuesta responde la pregunta?\n",
    "- **Context Utilization**: Â¿Se usa eficientemente el contexto?\n",
    "\n",
    "### ğŸ“Š Frameworks de EvaluaciÃ³n 2025\n",
    "\n",
    "**Herramientas Principales:**\n",
    "- **[RAGAS](https://github.com/explodinggradients/ragas)** - Framework completo de evaluaciÃ³n RAG\n",
    "- **[LangSmith](https://docs.smith.langchain.com/)** - Evaluaciones de LangChain para producciÃ³n\n",
    "- **[TruLens](https://www.trulens.org/)** - EvaluaciÃ³n de aplicaciones LLM\n",
    "- **[Arize Phoenix](https://phoenix.arize.com/)** - Observabilidad para ML/LLM\n",
    "\n",
    "### ğŸ§ª MetodologÃ­a de EvaluaciÃ³n Recomendada\n",
    "\n",
    "1. **Dataset Curado**: Crear preguntas representativas de usuarios reales\n",
    "2. **EvaluaciÃ³n A/B**: Comparar estrategias de chunking con mÃ©tricas objetivas  \n",
    "3. **EvaluaciÃ³n Humana**: Validar que las mÃ©tricas reflejen calidad real\n",
    "4. **AnÃ¡lisis de Costos**: Considerar trade-offs entre calidad y costo computacional\n",
    "\n",
    "### ğŸ’¡ Principio Fundamental 2025\n",
    "\n",
    "> **\"El objetivo del chunking no es dividir texto, sino crear unidades de informaciÃ³n que maximicen la recuperaciÃ³n relevante para responder preguntas de usuarios reales.\"**\n",
    "\n",
    "La evaluaciÃ³n debe reflejar casos de uso reales, no mÃ©tricas abstractas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tLkIomBTYVAE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16222,
     "status": "ok",
     "timestamp": 1720829105488,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "tLkIomBTYVAE",
    "outputId": "bfae1e95-66d5-4f3d-f224-bdc9805bb3b6"
   },
   "outputs": [],
   "source": [
    "# InstalaciÃ³n de Dependencias - Stack 2025\n",
    "# !pip install --upgrade langchain langchain-openai langchain-qdrant langchain-text-splitters tiktoken qdrant-client tqdm scikit-learn matplotlib plotly ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carga las variables de entorno desde un archivo .env si existe\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90lrwa4ezq",
   "metadata": {},
   "source": [
    "# ğŸ¤– Embeddings en 2025: Fundamentos Esenciales\n",
    "\n",
    "## Â¿QuÃ© es un Embedding?\n",
    "\n",
    "Un **embedding** es una representaciÃ³n numÃ©rica (vector) que captura el significado semÃ¡ntico de un texto. Imagina que cada palabra, oraciÃ³n o documento se convierte en una \"coordenada\" en un espacio multidimensional donde textos con significados similares estÃ¡n cerca unos de otros.\n",
    "\n",
    "```\n",
    "\"El gato estÃ¡ durmiendo\" â†’ [0.2, -0.4, 0.8, ..., 0.1]  (1536 nÃºmeros)\n",
    "\"El felino estÃ¡ descansando\" â†’ [0.3, -0.3, 0.7, ..., 0.2]  (muy similar al anterior)\n",
    "\"La computadora estÃ¡ rota\" â†’ [-0.1, 0.9, -0.2, ..., 0.8]  (muy diferente)\n",
    "```\n",
    "\n",
    "## Â¿Por QuÃ© Necesitamos Embeddings?\n",
    "\n",
    "### ğŸ¯ Problemas que Resuelven:\n",
    "\n",
    "1. **BÃºsqueda SemÃ¡ntica**: Encontrar \"automÃ³vil\" cuando buscas \"coche\"\n",
    "2. **Similitud de Significado**: Detectar que \"enorme\" y \"gigante\" son similares\n",
    "3. **Contexto**: Distinguir \"banco\" (instituciÃ³n financiera) vs \"banco\" (asiento)\n",
    "4. **Multiidioma**: Conectar conceptos entre diferentes idiomas\n",
    "\n",
    "### ğŸ” En RAG EspecÃ­ficamente:\n",
    "\n",
    "- **Retrieval**: Encontrar chunks relevantes para una pregunta\n",
    "- **Ranking**: Ordenar resultados por relevancia semÃ¡ntica  \n",
    "- **Filtering**: Eliminar chunks no relacionados\n",
    "- **Clustering**: Agrupar documentos similares\n",
    "\n",
    "## Modelos de Embeddings OpenAI 2025\n",
    "\n",
    "### ğŸš€ text-embedding-3-large (Flagship Model)\n",
    "- **Dimensiones**: 3,072 (reducible a 256-3072)\n",
    "- **Performance**: +64.6% en MTEB benchmark\n",
    "- **Uso**: Aplicaciones de producciÃ³n que requieren mÃ¡xima precisiÃ³n\n",
    "- **Precio**: $0.00013 / 1K tokens\n",
    "\n",
    "### âš¡ text-embedding-3-small (Optimizado)\n",
    "- **Dimensiones**: 1,536 (reducible a 256-1536) \n",
    "- **Performance**: Comparable a ada-002 pero mÃ¡s eficiente\n",
    "- **Uso**: Aplicaciones con restricciones de latencia/costo\n",
    "- **Precio**: $0.00002 / 1K tokens\n",
    "\n",
    "### ğŸ“Š ComparaciÃ³n con Modelos Anteriores:\n",
    "\n",
    "| Modelo | Dimensiones | MTEB Score | Costo (1K tokens) |\n",
    "|--------|-------------|------------|-------------------|\n",
    "| ada-002 | 1,536 | 61.0% | $0.0001 |\n",
    "| text-embedding-3-small | 1,536 | ~61% | $0.00002 |\n",
    "| text-embedding-3-large | 3,072 | 64.6% | $0.00013 |\n",
    "\n",
    "### ğŸ›ï¸ Nueva Funcionalidad: Dimensionalidad Reducible\n",
    "\n",
    "```python\n",
    "# Puedes reducir dimensiones para optimizar costo/rendimiento\n",
    "embedding_256d = openai.embeddings.create(\n",
    "    input=\"texto ejemplo\",\n",
    "    model=\"text-embedding-3-large\",\n",
    "    dimensions=256  # Â¡6x reducciÃ³n de tamaÃ±o!\n",
    ")\n",
    "```\n",
    "\n",
    "## Â¿CÃ³mo Elegir el Modelo Correcto?\n",
    "\n",
    "### ğŸ­ Para ProducciÃ³n â†’ `text-embedding-3-large`\n",
    "- MÃ¡xima precisiÃ³n\n",
    "- Mejor para bÃºsquedas complejas\n",
    "- Worth el costo adicional\n",
    "\n",
    "### ğŸš€ Para Prototipado/Volumen Alto â†’ `text-embedding-3-small`  \n",
    "- Excelente relaciÃ³n calidad-precio\n",
    "- 5x mÃ¡s barato que large\n",
    "- Perfecto para experimentaciÃ³n\n",
    "\n",
    "### ğŸ’° Para Casos de Costo CrÃ­tico â†’ Dimensionalidad Reducida\n",
    "- Usa 256-512 dimensiones\n",
    "- Mantiene >95% del performance\n",
    "- Ahorro significativo en almacenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª DemostraciÃ³n PrÃ¡ctica: CÃ³mo Funcionan los Embeddings\n",
    "\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7623936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo de embeddings mÃ¡s avanzado de 2025\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    dimensions=512,  # Reducido para optimizar costo, mantiene 95%+ performance, pueden probar con 3072, 1532, 1024 o 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c804cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textos de ejemplo para demostrar similitud semÃ¡ntica\n",
    "textos = [\n",
    "    \"El gato estÃ¡ durmiendo en el sofÃ¡\",\n",
    "    \"El felino descansa en el sillÃ³n\", \n",
    "    \"Mi coche es muy rÃ¡pido\",\n",
    "    \"El automÃ³vil tiene gran velocidad\",\n",
    "    \"Python es un lenguaje de programaciÃ³n\",\n",
    "    \"La serpiente se desliza por el jardÃ­n\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8720c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar embeddings\n",
    "print(\"ğŸ”„ Generando embeddings con text-embedding-3-large...\")\n",
    "vectors = embeddings_model.embed_documents(textos)\n",
    "\n",
    "print(f\"âœ… Generados {len(vectors)} embeddings de {len(vectors[0])} dimensiones cada uno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ec542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular similitudes\n",
    "similarities = cosine_similarity(vectors)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rmoedyb77ym",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resultados\n",
    "print(\"\\nğŸ” Matriz de Similitud Coseno (1.0 = idÃ©ntico, 0.0 = sin relaciÃ³n):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, texto1 in enumerate(textos):\n",
    "    print(f\"\\nğŸ“ '{texto1[:50]}...'\")\n",
    "    for j, texto2 in enumerate(textos):\n",
    "        if i != j:  # No comparar consigo mismo\n",
    "            sim = similarities[i][j]\n",
    "            emoji = \"ğŸŸ¢\" if sim > 0.7 else \"ğŸŸ¡\" if sim > 0.5 else \"ğŸ”´\"\n",
    "            print(f\"  {emoji} vs '{texto2[:50]}...': {sim:.3f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Observaciones:\")\n",
    "print(\"ğŸŸ¢ Verde (>0.7): Alta similitud semÃ¡ntica\")  \n",
    "print(\"ğŸŸ¡ Amarillo (>0.5): Similitud moderada\")\n",
    "print(\"ğŸ”´ Rojo (<0.6): Baja similitud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86976c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostrar bÃºsqueda semÃ¡ntica\n",
    "query = \"Â¿DÃ³nde estÃ¡ el animal domÃ©stico?\"\n",
    "query_vector = embeddings_model.embed_query(query)\n",
    "\n",
    "print(f\"\\nğŸ” BÃºsqueda para: '{query}'\")\n",
    "print(\"Resultados ordenados por relevancia:\")\n",
    "\n",
    "# Calcular similitudes con la query\n",
    "query_similarities = cosine_similarity([query_vector], vectors)[0]\n",
    "\n",
    "# Ordenar por relevancia\n",
    "resultados = list(zip(textos, query_similarities))\n",
    "resultados.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (texto, score) in enumerate(resultados[:3]):\n",
    "    print(f\"{i+1}. {score:.3f} - {texto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9663ad9-217c-46d3-befc-8ccabeba890b",
   "metadata": {
    "id": "a9663ad9-217c-46d3-befc-8ccabeba890b"
   },
   "source": [
    "# 1. Character Splitting - Fundamentos y Mejores PrÃ¡cticas 2025\n",
    "\n",
    "## Â¿QuÃ© es Character Splitting?\n",
    "\n",
    "Character Splitting es la forma mÃ¡s bÃ¡sica de dividir texto: creamos chunks de un tamaÃ±o especÃ­fico de N caracteres **o tokens**, sin importar el contenido o estructura.\n",
    "\n",
    "## ğŸš¨ Cambio Importante en 2025: Tokens vs Caracteres\n",
    "\n",
    "**âŒ Enfoque Obsoleto (Pre-2024):** Contar caracteres\n",
    "**âœ… Enfoque Moderno (2025):** Contar **tokens** usando **tiktoken**\n",
    "\n",
    "**ğŸ”— tiktoken:** [https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)  \n",
    "*tiktoken es la librerÃ­a oficial de OpenAI para contar y manipular tokens de manera eficiente y precisa en todos los modelos modernos.*\n",
    "\n",
    "### Â¿Por quÃ© Tokens?\n",
    "\n",
    "- **LÃ­mites de API**: OpenAI cobra y limita por **tokens**, no caracteres\n",
    "- **PrecisiÃ³n**: Un token â‰  un caracter (especialmente en idiomas no ingleses)\n",
    "- **Embeddings**: Los modelos procesan **tokens**, no caracteres\n",
    "\n",
    "```\n",
    "Ejemplo:\n",
    "\"Hola mundo\" = 5 caracteres, pero puede ser 2-3 tokens\n",
    "```\n",
    "\n",
    "## CuÃ¡ndo Usar Character/Token Splitting\n",
    "\n",
    "### âœ… **Casos Apropiados:**\n",
    "- **Prototipado inicial** y testing\n",
    "- **Documentos homogÃ©neos** sin estructura compleja\n",
    "- **Restricciones estrictas** de tamaÃ±o de chunk\n",
    "\n",
    "### âŒ **Evitar Para:**\n",
    "- **Aplicaciones de producciÃ³n** (usar Recursive o Semantic)\n",
    "- **Documentos estructurados** (cÃ³digo, Markdown, JSON)\n",
    "- **Casos donde la coherencia** semÃ¡ntica es crÃ­tica\n",
    "\n",
    "## ParÃ¡metros Clave 2025\n",
    "\n",
    "### ğŸ¯ **Chunk Size** \n",
    "- **Recomendado**: 1000-1500 tokens\n",
    "- **LÃ­mite OpenAI**: 8,191 tokens para text-embedding-3-*\n",
    "- **Trade-off**: MÃ¡s grande = mÃ¡s contexto, pero menos precisiÃ³n\n",
    "\n",
    "### ğŸ”— **Chunk Overlap**\n",
    "- **Recomendado**: 20% del chunk size (200-300 tokens)\n",
    "- **PropÃ³sito**: Prevenir pÃ©rdida de contexto en fronteras\n",
    "- **Trade-off**: MÃ¡s overlap = mÃ¡s redundancia, mayor costo\n",
    "\n",
    "## Ejemplo PrÃ¡ctico con Mejores PrÃ¡cticas 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96299fc-30f5-4edf-ac23-23a29f9c7282",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1720829137626,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "c96299fc-30f5-4edf-ac23-23a29f9c7282"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# ğŸ“„ Texto de Ejemplo - Actualizado para 2025\n",
    "\n",
    "text = \"\"\"\n",
    "GPT-4o es el modelo de inteligencia artificial mÃ¡s avanzado de OpenAI hasta 2025, representando un salto significativo en capacidades multimodales. Este modelo procesa y genera contenido en mÃºltiples modalidades (texto, audio, imagen) en tiempo real, con tiempos de respuesta comparables a la reacciÃ³n humana en conversaciones.\n",
    "\n",
    "GPT-4o integra sus diversas entradas y salidas bajo un modelo unificado, haciÃ©ndolo mÃ¡s rÃ¡pido, rentable y eficiente que sus predecesores. AdemÃ¡s, logra resultados de vanguardia en benchmarks multilingÃ¼es y de visiÃ³n, estableciendo nuevos rÃ©cords en reconocimiento de voz y traducciÃ³n.\n",
    "\n",
    "El modelo se lanzÃ³ en mayo de 2024 y OpenAI planea implementar inmediatamente las capacidades de imagen y texto de GPT-4o en ChatGPT, incluida su versiÃ³n gratuita, con el modo de voz disponible para usuarios de ChatGPT Plus en las prÃ³ximas semanas. TambiÃ©n planean hacer disponibles las capacidades de audio y video del modelo para socios limitados de API en las prÃ³ximas semanas.\n",
    "\n",
    "En su anuncio de lanzamiento, OpenAI seÃ±alÃ³ que las capacidades de GPT-4o presentan nuevos desafÃ­os de seguridad, y mencionÃ³ mitigaciones y limitaciones como resultado. El modelo representa el futuro de la interacciÃ³n humano-IA, permitiendo conversaciones mÃ¡s naturales y la capacidad de proporcionar sugerencias o respuestas basadas en cargas de fotos.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"ğŸ“Š EstadÃ­sticas del texto:\")\n",
    "print(f\"Caracteres: {len(text):,}\")\n",
    "print(f\"Palabras: {len(text.split()):,}\")\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "print(f\"Tokens: {len(encoding.encode(text)):,}\")\n",
    "print(f\"LÃ­neas: {len(text.splitlines()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcebb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Token-Based Splitting - MÃ©todo Moderno 2025\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def token_based_splitting(text, chunk_size_tokens=100, chunk_overlap_tokens=20):\n",
    "    \"\"\"\n",
    "    Divide texto basÃ¡ndose en TOKENS (no caracteres) usando tiktoken.\n",
    "    Esta es la mejor prÃ¡ctica para 2025.\n",
    "    \"\"\"\n",
    "    # Usar el encoding de GPT-4\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    \n",
    "    # Convertir texto a tokens\n",
    "    tokens = encoding.encode(text)\n",
    "    total_tokens = len(tokens)\n",
    "    \n",
    "    print(f\"ğŸ“Š Total de tokens: {total_tokens:,}\")\n",
    "    \n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    while start_idx < len(tokens):\n",
    "        # Determinar final del chunk\n",
    "        end_idx = min(start_idx + chunk_size_tokens, len(tokens))\n",
    "        \n",
    "        # Extraer tokens del chunk\n",
    "        chunk_tokens = tokens[start_idx:end_idx]\n",
    "        \n",
    "        # Convertir tokens de vuelta a texto\n",
    "        chunk_text = encoding.decode(chunk_tokens)\n",
    "        \n",
    "        chunks.append({\n",
    "            'chunk_id': len(chunks),\n",
    "            'token_start': start_idx,\n",
    "            'token_end': end_idx,\n",
    "            'token_count': len(chunk_tokens),\n",
    "            'char_count': len(chunk_text),\n",
    "            'text': chunk_text\n",
    "        })\n",
    "        \n",
    "        # Avanzar posiciÃ³n con overlap\n",
    "        start_idx += chunk_size_tokens - chunk_overlap_tokens\n",
    "        \n",
    "        # Evitar chunks infinitesimalmente pequeÃ±os al final\n",
    "        if len(tokens) - start_idx < chunk_size_tokens * 0.5:\n",
    "            break\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11fb88f-17ed-44c2-b4de-a8a527fe63c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1720829356119,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "f11fb88f-17ed-44c2-b4de-a8a527fe63c7",
    "outputId": "e8f521a2-7075-45b2-b342-a6da1ea2e7ef"
   },
   "outputs": [],
   "source": [
    "# ğŸ§ª Aplicar la funciÃ³n con parÃ¡metros optimizados para 2025\n",
    "chunk_size = 100  # tokens (no caracteres!)\n",
    "chunk_overlap = 20  # 20% del chunk size\n",
    "\n",
    "chunks = token_based_splitting(text, chunk_size, chunk_overlap)\n",
    "\n",
    "print(f\"\\nâœ… Generados {len(chunks)} chunks\")\n",
    "print(f\"ğŸ”— Overlap configurado: {chunk_overlap} tokens ({chunk_overlap/chunk_size*100:.1f}%)\")\n",
    "\n",
    "# Mostrar los primeros chunks con estadÃ­sticas detalladas\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nğŸ“„ Chunk #{chunk['chunk_id'] + 1}\")\n",
    "    print(f\"   ğŸ¯ Tokens: {chunk['token_count']}\")\n",
    "    print(f\"   ğŸ“ Caracteres: {chunk['char_count']}\")\n",
    "    print(f\"   ğŸ“ PosiciÃ³n: tokens {chunk['token_start']}-{chunk['token_end']}\")\n",
    "    print(f\"   ğŸ“– Contenido: {chunk['text']}...\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Ventaja del mÃ©todo basado en tokens:\")\n",
    "print(f\"   â€¢ PrecisiÃ³n en costos de API\")\n",
    "print(f\"   â€¢ Compatibility con lÃ­mites de embedding\")\n",
    "print(f\"   â€¢ Control exacto sobre el tamaÃ±o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85945f0-4a09-4bd9-bdb6-bafe03089053",
   "metadata": {
    "executionInfo": {
     "elapsed": 732,
     "status": "ok",
     "timestamp": 1720829258326,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "d85945f0-4a09-4bd9-bdb6-bafe03089053"
   },
   "outputs": [],
   "source": [
    "# ğŸ¦œ Character Splitting con LangChain - VersiÃ³n 2025\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbeb8d-c5a0-4047-8250-967313c20935",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1720829559830,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "3dcbeb8d-c5a0-4047-8250-967313c20935",
    "outputId": "f2ea2fa7-de35-40a0-8e31-649091c98603"
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ LangChain CharacterTextSplitter - ConfiguraciÃ³n Ã“ptima 2025\n",
    "\n",
    "# FunciÃ³n para contar tokens precisamente\n",
    "def count_tokens(text):\n",
    "    \"\"\"Cuenta tokens usando tiktoken - el mÃ©todo correcto para 2025\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# ParÃ¡metros optimizados para 2025\n",
    "chunk_size = 100      # Caracteres aproximados (usaremos length_function para tokens)\n",
    "chunk_overlap = 80    # ~20% overlap\n",
    "separator = '\\n\\n'    # Preferir divisiones por pÃ¡rrafo cuando sea posible\n",
    "\n",
    "# âœ… CONFIGURACIÃ“N MODERNA: Usar length_function basada en tokens\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator=separator,\n",
    "    strip_whitespace=True,\n",
    "    # ğŸš€ NOVEDAD 2025: Usar funciÃ³n de conteo de tokens\n",
    "    length_function=count_tokens  # Â¡Esto es clave!\n",
    ")\n",
    "\n",
    "# Crear documentos\n",
    "documentos = text_splitter.create_documents([text])\n",
    "\n",
    "print(f\"âœ… Creados {len(documentos)} chunks usando CharacterTextSplitter\")\n",
    "print(f\"ğŸ›ï¸ ConfiguraciÃ³n: chunk_size={chunk_size}, overlap={chunk_overlap}, separator='{separator}'\")\n",
    "print(f\"ğŸ“Š FunciÃ³n de longitud: Conteo de tokens (no caracteres)\")\n",
    "\n",
    "# Analizar los chunks generados\n",
    "for i, doc in enumerate(documentos[:3]):\n",
    "    content = doc.page_content\n",
    "    token_count = count_tokens(content)\n",
    "    char_count = len(content)\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Chunk {i+1}:\")\n",
    "    print(f\"   ğŸ¯ Tokens: {token_count}\")\n",
    "    print(f\"   ğŸ“ Caracteres: {char_count}\")\n",
    "    print(f\"   ğŸ“– Contenido: {content[:120]}...\")\n",
    "\n",
    "# ğŸ’¡ ComparaciÃ³n: Character counting vs Token counting\n",
    "print(f\"\\nğŸ” ComparaciÃ³n de MÃ©todos:\")\n",
    "total_chars = sum(len(doc.page_content) for doc in documentos)\n",
    "total_tokens = sum(count_tokens(doc.page_content) for doc in documentos)\n",
    "\n",
    "print(f\"   ğŸ“ Total caracteres: {total_chars:,}\")\n",
    "print(f\"   ğŸ¯ Total tokens: {total_tokens:,}\")\n",
    "print(f\"   ğŸ“Š Ratio tokens/char: {total_tokens/total_chars:.2f}\")\n",
    "\n",
    "print(f\"\\nğŸ’° Implicaciones de Costo:\")\n",
    "embedding_cost_per_1k_tokens = 0.00013  # text-embedding-3-large\n",
    "estimated_cost = (total_tokens / 1000) * embedding_cost_per_1k_tokens\n",
    "print(f\"   ğŸ’µ Costo estimado embeddings: ${estimated_cost:.4f}\")\n",
    "print(f\"   ğŸ“ˆ Con {len(documentos)} chunks Ã— 3,072 dimensiones = {len(documentos) * 3072:,} nÃºmeros almacenados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4aaa8-b90b-499e-b2d5-bc623b5bb751",
   "metadata": {
    "id": "dcd4aaa8-b90b-499e-b2d5-bc623b5bb751"
   },
   "source": [
    "## ğŸ”¬ VisualizaciÃ³n Interactiva\n",
    "\n",
    "ğŸ’¡ **RecomendaciÃ³n**: Visualiza tus estrategias de chunking en **[ChunkViz.com](http://www.chunkviz.com)** para entender mejor cÃ³mo diferentes parÃ¡metros afectan la divisiÃ³n del texto.\n",
    "\n",
    "**Nota sobre Character Splitting en 2025:**\n",
    "- âœ… **Ãštil para**: ExperimentaciÃ³n rÃ¡pida, casos simples\n",
    "- âŒ **Evitar para**: ProducciÃ³n, documentos complejos\n",
    "- ğŸ¯ **PrÃ³ximo paso**: Recursive Character Splitting para mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ce9aa-17c3-4205-b433-2eae612c2225",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1720790523896,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "814ce9aa-17c3-4205-b433-2eae612c2225",
    "outputId": "60afbbde-ea8f-4fbd-85d7-41e68548d890"
   },
   "outputs": [],
   "source": [
    "# Probemos usando un separador\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0,\n",
    "    separator='.',\n",
    "    strip_whitespace=True #\n",
    "  )\n",
    "\n",
    "# Luego, podemos dividir nuestro texto utilizando create_documents. Nota: create_documents espera una lista de textos,\n",
    "# asÃ­ que si solo tienes una cadena (como nosotros), necesitarÃ¡s envolverla en []\n",
    "\n",
    "documentos = text_splitter.create_documents([text])\n",
    "\n",
    "for doc in documentos:\n",
    "    print(f\"\\nğŸ“„ Doc:\")\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0862aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539aa1a2-67b8-4585-b0d3-306703ea856b",
   "metadata": {
    "id": "539aa1a2-67b8-4585-b0d3-306703ea856b"
   },
   "source": [
    "# 2. Recursive Character Text Splitting - El EstÃ¡ndar de 2025 â­\n",
    "\n",
    "## Â¿Por quÃ© es el \"Gold Standard\" para Texto?\n",
    "\n",
    "Recursive Character Text Splitting resuelve las limitaciones del character splitting bÃ¡sico al **respetar la estructura natural** del texto. En lugar de cortar arbitrariamente, trata de dividir en puntos lÃ³gicos siguiendo una jerarquÃ­a de separadores.\n",
    "\n",
    "## ğŸ§  CÃ³mo Funciona la LÃ³gica Recursiva\n",
    "\n",
    "1. **Intenta dividir por pÃ¡rrafos** (`\\n\\n`)\n",
    "2. Si el chunk es muy grande â†’ **divide por lÃ­neas** (`\\n`)\n",
    "3. Si aÃºn es muy grande â†’ **divide por oraciones** (`. `)\n",
    "4. Si aÃºn es muy grande â†’ **divide por palabras** (` `)\n",
    "5. Como Ãºltimo recurso â†’ **divide por caracteres** (``)\n",
    "\n",
    "## ğŸ“‹ Separadores por Defecto en LangChain 2025\n",
    "\n",
    "Los separadores estÃ¡n **ordenados por prioridad** (se prueba de izquierda a derecha):\n",
    "\n",
    "```python\n",
    "separators = [\n",
    "    \"\\n\\n\",    # PÃ¡rrafos (mÃ¡xima prioridad)\n",
    "    \"\\n\",      # LÃ­neas  \n",
    "    \" \",       # Espacios entre palabras\n",
    "    \"\"         # Caracteres individuales (Ãºltimo recurso)\n",
    "]\n",
    "```\n",
    "\n",
    "### ğŸ†• Separadores Adicionales Recomendados para 2025:\n",
    "\n",
    "```python\n",
    "# Para contenido web/HTML\n",
    "\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \" \", \"\"\n",
    "\n",
    "# Para contenido tÃ©cnico/cÃ³digo  \n",
    "\"\\n\\n\", \"\\n\", \"```\", \"```python\", \"```javascript\", \". \", \" \", \"\"\n",
    "\n",
    "# Para idioma espaÃ±ol (optimizaciÃ³n)\n",
    "\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"\n",
    "```\n",
    "\n",
    "## ğŸ¯ ParÃ¡metros Optimizados 2025\n",
    "\n",
    "### ConfiguraciÃ³n EstÃ¡ndar Recomendada:\n",
    "- **chunk_size**: 1000-1500 tokens\n",
    "- **chunk_overlap**: 200-300 tokens (20% del chunk_size)\n",
    "- **length_function**: Conteo de tokens (tiktoken)\n",
    "- **is_separator_regex**: False (usar strings literales)\n",
    "\n",
    "### Para Diferentes Casos de Uso:\n",
    "\n",
    "| Caso | Chunk Size | Overlap | Separadores |\n",
    "|------|------------|---------|-------------|\n",
    "| ğŸ“° ArtÃ­culos/Blogs | 800-1200 | 150-200 | PÃ¡rrafos + oraciones |\n",
    "| ğŸ“š DocumentaciÃ³n | 1200-1800 | 200-300 | Secciones + pÃ¡rrafos |\n",
    "| ğŸ’¬ Chat/Conversaciones | 400-600 | 80-100 | LÃ­neas + oraciones |\n",
    "| ğŸ“„ Documentos Legales | 1500-2000 | 300-400 | PÃ¡rrafos largos |\n",
    "\n",
    "## Ejemplo PrÃ¡ctico con Optimizaciones 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772695d-0c5e-4e19-bb69-14e9bd7a15a7",
   "metadata": {
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1720829595508,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "0772695d-0c5e-4e19-bb69-14e9bd7a15a7"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ğŸ“š Texto de Ejemplo Extenso - Contenido Estructurado 2025\n",
    "\n",
    "text_largo = \"\"\"\n",
    "# La EvoluciÃ³n de la Inteligencia Artificial en 2025\n",
    "\n",
    "## IntroducciÃ³n\n",
    "\n",
    "La inteligencia artificial ha experimentado transformaciones revolucionarias desde 2022. GPT-4o, lanzado por OpenAI en mayo de 2024, representa un hito en la evoluciÃ³n de los modelos multimodales. Este modelo no solo procesa texto, sino que integra capacidades de audio, imagen y video en tiempo real.\n",
    "\n",
    "## Capacidades Multimodales\n",
    "\n",
    "### Procesamiento de Texto\n",
    "GPT-4o mantiene todas las capacidades de procesamiento de texto de sus predecesores, pero con mejoras significativas en coherencia y contexto. El modelo puede manejar conversaciones largas manteniendo el hilo narrativo de manera mÃ¡s efectiva.\n",
    "\n",
    "### Capacidades de Audio\n",
    "Una de las innovaciones mÃ¡s importantes es la capacidad de procesar audio en tiempo real. Esto permite conversaciones naturales con tiempos de respuesta comparables a la interacciÃ³n humana.\n",
    "\n",
    "### Procesamiento Visual\n",
    "El modelo puede analizar imÃ¡genes, describir contenido visual, y extraer informaciÃ³n de screenshots. Esta capacidad es especialmente Ãºtil para aplicaciones educativas y profesionales.\n",
    "\n",
    "## Impacto en Diferentes Industrias\n",
    "\n",
    "### Sector Educativo\n",
    "Los educadores estÃ¡n utilizando IA para crear contenido personalizado, evaluar estudiantes de manera automÃ¡tica, y proporcionar tutorÃ­as individualizadas. Las universidades estÃ¡n integrando herramientas de IA en sus currÃ­culos.\n",
    "\n",
    "### Industria de la Salud  \n",
    "Los profesionales mÃ©dicos aprovechan la IA para diagnÃ³sticos mÃ¡s precisos, anÃ¡lisis de imÃ¡genes mÃ©dicas, y desarrollo de tratamientos personalizados. Sin embargo, persisten preocupaciones sobre la precisiÃ³n y responsabilidad.\n",
    "\n",
    "### TecnologÃ­a y Desarrollo\n",
    "Los desarrolladores utilizan IA para escribir cÃ³digo, detectar errores, y optimizar rendimiento. Herramientas como GitHub Copilot han revolucionado la forma en que se desarrolla software.\n",
    "\n",
    "## DesafÃ­os y Consideraciones Ã‰ticas\n",
    "\n",
    "### Privacidad de Datos\n",
    "La recopilaciÃ³n masiva de datos para entrenar modelos IA plantea serias preocupaciones sobre privacidad. Las empresas deben balance la innovaciÃ³n con la protecciÃ³n del usuario.\n",
    "\n",
    "### Sesgo AlgorÃ­tmico\n",
    "Los modelos pueden perpetuar sesgos presentes en los datos de entrenamiento. Es crucial desarrollar tÃ©cnicas para detectar y mitigar estos sesgos.\n",
    "\n",
    "### Impacto Laboral\n",
    "Mientras la IA automatiza ciertas tareas, tambiÃ©n crea nuevas oportunidades laborales. La sociedad debe adaptarse a estos cambios preparando a los trabajadores para roles emergentes.\n",
    "\n",
    "## Tendencias Futuras\n",
    "\n",
    "### EspecializaciÃ³n por Dominio\n",
    "Esperamos ver modelos mÃ¡s especializados para dominios especÃ­ficos como medicina, derecho, y ciencias. Estos modelos ofrecerÃ¡n mayor precisiÃ³n en sus Ã¡reas de especializaciÃ³n.\n",
    "\n",
    "### Eficiencia EnergÃ©tica\n",
    "La sustentabilidad se estÃ¡ convirtiendo en prioridad. Los investigadores trabajan en modelos mÃ¡s eficientes que requieren menos recursos computacionales.\n",
    "\n",
    "### IntegraciÃ³n Ubicua\n",
    "La IA se integrarÃ¡ mÃ¡s profundamente en dispositivos cotidianos, desde electrodomÃ©sticos hasta vehÃ­culos autÃ³nomos.\n",
    "\n",
    "## ConclusiÃ³n\n",
    "\n",
    "El 2025 marca un punto de inflexiÃ³n en la adopciÃ³n de IA. Mientras celebramos los avances, debemos abordar responsablemente los desafÃ­os que presenta esta tecnologÃ­a transformadora.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"ğŸ“Š EstadÃ­sticas del texto extenso:\")\n",
    "print(f\"Caracteres: {len(text_largo):,}\")\n",
    "print(f\"Palabras: {len(text_largo.split()):,}\") \n",
    "print(f\"PÃ¡rrafos: {len(text_largo.split('\\\\n\\\\n')):,}\")\n",
    "print(f\"Secciones: {text_largo.count('#'):,}\")\n",
    "\n",
    "# Contar tokens usando tiktoken\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "token_count = len(encoding.encode(text_largo))\n",
    "print(f\"Tokens: {token_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb158c-6bbe-4f49-95df-a8b43965a566",
   "metadata": {
    "id": "9fbb158c-6bbe-4f49-95df-a8b43965a566"
   },
   "source": [
    "Now let's make our text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ RecursiveCharacterTextSplitter - ConfiguraciÃ³n Optimizada 2025\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# FunciÃ³n de conteo de tokens (mejor prÃ¡ctica 2025)\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# ğŸ¯ CONFIGURACIÃ“N OPTIMIZADA PARA 2025\n",
    "# Definir valores de configuraciÃ³n primero\n",
    "chunk_size_value = 200\n",
    "chunk_overlap_value = 100\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size_value,       # Tokens (usando length_function)\n",
    "    chunk_overlap=chunk_overlap_value, # 20% overlap\n",
    "    length_function=count_tokens,      # Â¡CLAVE! Usar tokens no caracteres\n",
    "    separators=[                       # Separadores optimizados para espaÃ±ol\n",
    "        \"\\n# \",                        # TÃ­tulos de secciÃ³n\n",
    "        \"\\n## \",                       # SubtÃ­tulos  \n",
    "        \"\\n### \",                      # Sub-subtÃ­tulos\n",
    "        \"\\n\\n\",                        # PÃ¡rrafos\n",
    "        \"\\n\",                          # LÃ­neas\n",
    "        \". \",                          # Final de oraciÃ³n\n",
    "        \"! \",                          # Exclamaciones\n",
    "        \"? \",                          # Preguntas\n",
    "        \"; \",                          # Punto y coma\n",
    "        \", \",                          # Comas\n",
    "        \" \",                           # Espacios\n",
    "        \"\"                             # Caracteres (Ãºltimo recurso)\n",
    "    ],\n",
    "    keep_separator=True,               # Preservar separadores para contexto\n",
    "    strip_whitespace=True,             # Limpiar espacios en blanco\n",
    "    is_separator_regex=False           # Usar strings literales (mÃ¡s rÃ¡pido)\n",
    ")\n",
    "\n",
    "# Crear chunks\n",
    "documentos = text_splitter.create_documents([text_largo])\n",
    "\n",
    "print(f\"âœ… RecursiveCharacterTextSplitter creÃ³ {len(documentos)} chunks\")\n",
    "print(f\"ğŸ›ï¸ ConfiguraciÃ³n: {chunk_size_value} tokens/chunk, {chunk_overlap_value} overlap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67037b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ec54c4-bda6-4254-97dd-983775b1d729",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1720829602830,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "03ec54c4-bda6-4254-97dd-983775b1d729",
    "outputId": "cb1fae4b-d637-40fc-f12e-de81f4c69ae6"
   },
   "outputs": [],
   "source": [
    "# AnÃ¡lisis detallado de los chunks\n",
    "total_tokens = 0\n",
    "chunks_analysis = []\n",
    "\n",
    "for i, doc in enumerate(documentos):\n",
    "    content = doc.page_content\n",
    "    token_count = count_tokens(content)\n",
    "    char_count = len(content)\n",
    "    \n",
    "    # Detectar tipo de contenido por el separador principal\n",
    "    content_type = \"pÃ¡rrafo\"\n",
    "    if content.strip().startswith('#'):\n",
    "        content_type = \"secciÃ³n\"\n",
    "    elif '. ' in content and len(content.split('. ')) > 3:\n",
    "        content_type = \"multi-oraciÃ³n\"\n",
    "    elif '\\n' in content and not content.strip().startswith('#'):\n",
    "        content_type = \"multi-lÃ­nea\"\n",
    "    \n",
    "    chunks_analysis.append({\n",
    "        'chunk': i + 1,\n",
    "        'tokens': token_count,\n",
    "        'chars': char_count,\n",
    "        'type': content_type,\n",
    "        'preview': content[:100].replace('\\n', ' ')\n",
    "    })\n",
    "    \n",
    "    total_tokens += token_count\n",
    "\n",
    "# Mostrar anÃ¡lisis de los primeros chunks\n",
    "print(f\"\\nğŸ“Š AnÃ¡lisis de Chunks (primeros 5):\")\n",
    "for chunk in chunks_analysis[:5]:\n",
    "    print(f\"ğŸ“„ Chunk {chunk['chunk']} ({chunk['type']}):\")\n",
    "    print(f\"   ğŸ¯ {chunk['tokens']} tokens | ğŸ“ {chunk['chars']} chars\")\n",
    "    print(f\"   ğŸ“– {chunk['preview']}...\")\n",
    "    print()\n",
    "\n",
    "# EstadÃ­sticas generales\n",
    "avg_tokens_per_chunk = total_tokens / len(documentos)\n",
    "efficiency = (total_tokens / count_tokens(text_largo)) * 100\n",
    "\n",
    "print(f\"ğŸ“ˆ EstadÃ­sticas Generales:\")\n",
    "print(f\"   ğŸ¯ Total tokens en chunks: {total_tokens:,}\")\n",
    "print(f\"   ğŸ“Š Promedio tokens/chunk: {avg_tokens_per_chunk:.1f}\")\n",
    "print(f\"   âš¡ Eficiencia (sin overlap): {efficiency:.1f}%\")\n",
    "print(f\"   ğŸ’° Costo estimado embeddings: ${(total_tokens/1000) * 0.00013:.4f}\")\n",
    "\n",
    "# Detectar separadores utilizados\n",
    "print(f\"\\nğŸ” Tipos de DivisiÃ³n Detectados:\")\n",
    "for chunk_type in set(chunk['type'] for chunk in chunks_analysis):\n",
    "    count = sum(1 for chunk in chunks_analysis if chunk['type'] == chunk_type)\n",
    "    print(f\"   ğŸ“ {chunk_type.capitalize()}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ov2lf5e3jt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¬ ComparaciÃ³n de Configuraciones - A/B Testing 2025\n",
    "\n",
    "# Definir diferentes configuraciones para comparar\n",
    "configs = [\n",
    "    {\n",
    "        \"name\": \"ğŸ† Optimizada 2025\",\n",
    "        \"chunk_size\": 800,\n",
    "        \"chunk_overlap\": 160,\n",
    "        \"separators\": [\"\\n# \", \"\\n## \", \"\\n### \", \"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ğŸ“Š Conservadora\", \n",
    "        \"chunk_size\": 1200,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"separators\": [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"âš¡ Eficiente\",\n",
    "        \"chunk_size\": 500,\n",
    "        \"chunk_overlap\": 50,\n",
    "        \"separators\": [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for config in configs:\n",
    "    # Crear splitter con la configuraciÃ³n\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config[\"chunk_size\"],\n",
    "        chunk_overlap=config[\"chunk_overlap\"], \n",
    "        length_function=count_tokens,\n",
    "        separators=config[\"separators\"],\n",
    "        keep_separator=True,\n",
    "        strip_whitespace=True\n",
    "    )\n",
    "    \n",
    "    # Aplicar al texto\n",
    "    docs = splitter.create_documents([text_largo])\n",
    "    \n",
    "    # Calcular mÃ©tricas\n",
    "    total_tokens = sum(count_tokens(doc.page_content) for doc in docs)\n",
    "    avg_tokens = total_tokens / len(docs) if docs else 0\n",
    "    cost = (total_tokens / 1000) * 0.00013  # text-embedding-3-large\n",
    "    \n",
    "    # Evaluar calidad (chunks que preservan estructura completa)\n",
    "    quality_chunks = 0\n",
    "    for doc in docs:\n",
    "        content = doc.page_content.strip()\n",
    "        if content.startswith('#') or '. ' in content:\n",
    "            quality_chunks += 1\n",
    "    \n",
    "    quality_ratio = (quality_chunks / len(docs)) * 100 if docs else 0\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'config': config[\"name\"],\n",
    "        'chunks': len(docs),\n",
    "        'avg_tokens': avg_tokens,\n",
    "        'total_tokens': total_tokens,\n",
    "        'cost': cost,\n",
    "        'quality': quality_ratio\n",
    "    })\n",
    "\n",
    "# Mostrar tabla de comparaciÃ³n\n",
    "print(\"ğŸ COMPARACIÃ“N DE CONFIGURACIONES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Config':<20} {'Chunks':<8} {'Avg Tokens':<12} {'Total':<8} {'Costo':<10} {'Calidad%':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for result in comparison_results:\n",
    "    print(f\"{result['config']:<20} {result['chunks']:<8} {result['avg_tokens']:<12.0f} \"\n",
    "          f\"{result['total_tokens']:<8} ${result['cost']:<9.4f} {result['quality']:<10.1f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ InterpretaciÃ³n de Resultados:\")\n",
    "print(\"ğŸ“Š Chunks: Menos chunks = menos llamadas a embedding, pero puede perder granularidad\")\n",
    "print(\"ğŸ¯ Avg Tokens: Chunks mÃ¡s grandes = mÃ¡s contexto, pero menos precisiÃ³n\")\n",
    "print(\"ğŸ’° Costo: Calculado para text-embedding-3-large\")\n",
    "print(\"ğŸ† Calidad%: % de chunks que preservan estructura completa (secciones/oraciones)\")\n",
    "\n",
    "# RecomendaciÃ³n basada en resultados\n",
    "best_quality = max(comparison_results, key=lambda x: x['quality'])\n",
    "most_efficient = min(comparison_results, key=lambda x: x['cost'])\n",
    "\n",
    "print(f\"\\nğŸ¯ Recomendaciones:\")\n",
    "print(f\"   ğŸ† Mayor calidad: {best_quality['config']} ({best_quality['quality']:.1f}% calidad)\")\n",
    "print(f\"   ğŸ’° MÃ¡s eficiente: {most_efficient['config']} (${most_efficient['cost']:.4f} costo)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8734e-47da-4a08-8459-9bf8bfed7fe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1720829691168,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "6da8734e-47da-4a08-8459-9bf8bfed7fe4",
    "outputId": "38ee1d70-f02a-4100-d490-b32fd8c982bf"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400\n",
    "  )\n",
    "\n",
    "documentos = text_splitter.create_documents([text_largo])\n",
    "\n",
    "for i, doc in enumerate(documentos):\n",
    "  chunk_text = doc.page_content\n",
    "  print(f\"\\nğŸ“„ Chunk {i+1} (chars: {len(chunk_text)}, preview: {chunk_text[:60].replace(chr(10), ' ')}...)\")\n",
    "  print(\"-\" * 80)\n",
    "  print(chunk_text[:400] + (\"...\" if len(chunk_text) > 400 else \"\"))\n",
    "  print(f\"\\nğŸ”¢ Chunk size (chars): {len(chunk_text)}\")\n",
    "  print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f32a73-0c8a-498c-a3a1-3e7dba4658c9",
   "metadata": {
    "id": "c5f32a73-0c8a-498c-a3a1-3e7dba4658c9"
   },
   "source": [
    "# 3. Document-Specific Splitting - EspecializaciÃ³n por Formato 2025\n",
    "\n",
    "## La Importancia de la EspecializaciÃ³n\n",
    "\n",
    "En 2025, los sistemas RAG manejan **documentos cada vez mÃ¡s diversos**. No todos los textos son iguales - una pÃ¡gina web tiene estructura diferente a cÃ³digo Python, un JSON difiere de un Markdown, y un CSV requiere tratamiento especializado.\n",
    "\n",
    "**Document-Specific Splitting** aprovecha la **estructura inherente** de cada formato para crear chunks mÃ¡s inteligentes y contextualmente relevantes.\n",
    "\n",
    "## ğŸ¯ Formatos Soportados en LangChain 2025\n",
    "\n",
    "### Texto y Documentos\n",
    "- **MarkdownTextSplitter**: Respeta headers, cÃ³digo, listas\n",
    "- **LaTeXTextSplitter**: Para documentos acadÃ©micos/cientÃ­ficos \n",
    "- **NLTKTextSplitter**: DivisiÃ³n avanzada por oraciones\n",
    "\n",
    "### CÃ³digo y TÃ©cnico  \n",
    "- **PythonCodeTextSplitter**: Preserva funciones, clases\n",
    "- **JavaScriptTextSplitter**: Mantiene funciones, objetos\n",
    "- **SolidityTextSplitter**: Para contratos inteligentes\n",
    "\n",
    "### Web y Estructurado\n",
    "- **HTMLHeaderTextSplitter**: Usa tags HTML como separadores\n",
    "- **JSONTextSplitter**: Divide por estructura JSON\n",
    "- **CSVTextSplitter**: Maneja filas y columnas\n",
    "\n",
    "### Nuevos en 2025 ğŸ†•\n",
    "- **XMLTextSplitter**: Para documentos XML complejos\n",
    "- **YAMLTextSplitter**: Configuraciones y metadatos\n",
    "- **TableTextSplitter**: Tablas especializadas con contexto\n",
    "\n",
    "## Ventajas del Splitting Especializado\n",
    "\n",
    "### âœ… **Preserva Contexto**\n",
    "```python\n",
    "# âŒ Splitting genÃ©rico podrÃ­a dividir asÃ­:\n",
    "\"def calcular_precio(producto):\\n    return producto.precio * 1.2\\n\\n# Esta funciÃ³n calcula\"\n",
    "\n",
    "# âœ… Splitting especializado mantiene funciones completas:\n",
    "\"def calcular_precio(producto):\\n    return producto.precio * 1.21\\n    # Incluye IVA del 21%\"\n",
    "```\n",
    "\n",
    "### âœ… **Mejora BÃºsqueda**\n",
    "Los chunks especializados contienen **unidades semÃ¡nticas completas** (funciones, secciones, objetos JSON), mejorando la relevancia en retrieval.\n",
    "\n",
    "### âœ… **Reduce Ruido** \n",
    "Evita chunks fragmentados que confunden al modelo durante la generaciÃ³n.\n",
    "\n",
    "## Ejemplos PrÃ¡cticos por Formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2333028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ Ejemplo 1: Markdown - Splitting Inteligente por Estructura\n",
    "\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# Markdown mÃ¡s complejo y realista para 2025\n",
    "markdown_text = \"\"\"\n",
    "# GuÃ­a Completa de RAG en 2025\n",
    "\n",
    "## IntroducciÃ³n a RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combina la potencia de los modelos de lenguaje con bases de conocimiento externas. En 2025, RAG se ha convertido en el estÃ¡ndar para aplicaciones que requieren informaciÃ³n actualizada y especÃ­fica.\n",
    "\n",
    "### Beneficios Principales\n",
    "\n",
    "- **InformaciÃ³n Actualizada**: Acceso a datos recientes no incluidos en el entrenamiento\n",
    "- **ReducciÃ³n de Alucinaciones**: Respuestas basadas en fuentes verificables  \n",
    "- **EspecializaciÃ³n de Dominio**: Conocimiento especÃ­fico sin reentrenamiento\n",
    "\n",
    "### Arquitectura TÃ­pica\n",
    "\n",
    "```python\n",
    "# Pipeline RAG bÃ¡sico\n",
    "def rag_pipeline(query):\n",
    "    # 1. Embedding de la query\n",
    "    query_embedding = embedder.embed(query)\n",
    "    \n",
    "    # 2. BÃºsqueda en vector store\n",
    "    relevant_docs = vector_store.search(query_embedding, k=5)\n",
    "    \n",
    "    # 3. GeneraciÃ³n con contexto\n",
    "    response = llm.generate(query, context=relevant_docs)\n",
    "    return response\n",
    "```\n",
    "\n",
    "## Componentes Esenciales\n",
    "\n",
    "### 1. Document Loading\n",
    "\n",
    "Los document loaders extraen contenido de fuentes diversas:\n",
    "\n",
    "- **PDFLoader**: Para documentos PDF\n",
    "- **WebBaseLoader**: Para pÃ¡ginas web\n",
    "- **DirectoryLoader**: Para carpetas completas\n",
    "\n",
    "### 2. Text Splitting\n",
    "\n",
    "```python\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=count_tokens\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Vector Storage\n",
    "\n",
    "#### Opciones Populares 2025:\n",
    "\n",
    "| Vector Store | Pros | Contras |\n",
    "|--------------|------|---------|\n",
    "| Qdrant | Alto rendimiento | Requiere infraestructura |\n",
    "| Pinecone | Managed service | Costoso para volÃºmenes altos |\n",
    "| FAISS | Gratuito | Solo local/en memoria |\n",
    "\n",
    "## Mejores PrÃ¡cticas 2025\n",
    "\n",
    "> **Tip Importante**: Siempre evalÃºa tu pipeline RAG con mÃ©tricas objetivas como RAGAS antes de producciÃ³n.\n",
    "\n",
    "### Chunking Strategy\n",
    "1. Usa splitting especializado por tipo de documento\n",
    "2. Optimiza chunk size para tu caso de uso (800-1500 tokens tÃ­picamente)\n",
    "3. Considera embedding cost vs retrieval quality\n",
    "\n",
    "### Embedding Selection\n",
    "- **OpenAI text-embedding-3-large**: Mejor calidad, mÃ¡s costoso\n",
    "- **OpenAI text-embedding-3-small**: Balance calidad-precio\n",
    "- **Local models**: Considera privacy vs performance\n",
    "\n",
    "## Casos de Uso Avanzados\n",
    "\n",
    "### RAG Conversacional\n",
    "Mantener contexto a travÃ©s de mÃºltiples turnos de conversaciÃ³n.\n",
    "\n",
    "### RAG Multimodal  \n",
    "Integrar texto, imÃ¡genes y audio en un sistema unificado.\n",
    "\n",
    "### RAG Agentic\n",
    "Usar LLMs para decidir dinÃ¡micamente quÃ© informaciÃ³n recuperar.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar MarkdownTextSplitter con parÃ¡metros optimizados\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "splitter = MarkdownTextSplitter(\n",
    "    chunk_size=200,              # Tokens optimizados para secciones\n",
    "    chunk_overlap=100,           # Mantener contexto entre chunks\n",
    "    length_function=count_tokens,\n",
    "    strip_whitespace=True\n",
    ")\n",
    "\n",
    "# Aplicar splitting\n",
    "documentos = splitter.create_documents([markdown_text])\n",
    "\n",
    "print(f\"ğŸ“„ MarkdownTextSplitter generÃ³ {len(documentos)} chunks\")\n",
    "\n",
    "documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba14168-451b-4e9c-b1d0-d1eac6996ad3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1720829759332,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "1ba14168-451b-4e9c-b1d0-d1eac6996ad3",
    "outputId": "d79889b5-9b26-495d-a2c7-6a60262db951"
   },
   "outputs": [],
   "source": [
    "# Analizar estructura preservada\n",
    "for i, doc in enumerate(documentos):\n",
    "    content = doc.page_content.strip()\n",
    "    token_count = count_tokens(content)\n",
    "    \n",
    "    # Detectar tipo de estructura\n",
    "    if content.startswith('# '):\n",
    "        chunk_type = \"ğŸ“œ TÃ­tulo Principal\" \n",
    "    elif content.startswith('## '):\n",
    "        chunk_type = \"ğŸ“‘ SecciÃ³n\"\n",
    "    elif content.startswith('### '):\n",
    "        chunk_type = \"ğŸ“‹ SubsecciÃ³n\"\n",
    "    elif content.startswith('```'):\n",
    "        chunk_type = \"ğŸ’» Bloque de CÃ³digo\"\n",
    "    elif content.startswith('| '):\n",
    "        chunk_type = \"ğŸ“Š Tabla\"\n",
    "    elif content.startswith('- ') or content.startswith('1. '):\n",
    "        chunk_type = \"ğŸ“ Lista\"\n",
    "    else:\n",
    "        chunk_type = \"ğŸ“„ PÃ¡rrafo\"\n",
    "    \n",
    "    print(f\"\\n{chunk_type} - Chunk {i+1} ({token_count} tokens):\")\n",
    "    preview = content.replace('\\n', ' ')[:120]\n",
    "    print(f\"   ğŸ“– {preview}...\")\n",
    "\n",
    "print(f\"\\nâœ… Ventajas del MarkdownTextSplitter:\")\n",
    "print(f\"   ğŸ¯ Respeta la jerarquÃ­a de headers (H1, H2, H3)\")\n",
    "print(f\"   ğŸ’» Preserva bloques de cÃ³digo completos\")\n",
    "print(f\"   ğŸ“Š Mantiene tablas Ã­ntegras\")\n",
    "print(f\"   ğŸ“ Agrupa listas relacionadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸŒ Ejemplo 2: HTML - Splitting por Estructura Web\n",
    "\n",
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "\n",
    "# HTML realista con estructura tÃ­pica de documentaciÃ³n web 2025\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <title>API Documentation - RAG Services 2025</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>API de Servicios RAG - DocumentaciÃ³n v3.0</h1>\n",
    "    \n",
    "    <h2>AutenticaciÃ³n</h2>\n",
    "    <p>Todos los endpoints requieren autenticaciÃ³n mediante API key. Incluye tu clave en el header <code>Authorization: Bearer YOUR_API_KEY</code>.</p>\n",
    "    \n",
    "    <h3>Obtener API Key</h3>\n",
    "    <p>Para obtener tu API key:</p>\n",
    "    <ol>\n",
    "        <li>RegÃ­strate en <a href=\"https://rag-services.com\">rag-services.com</a></li>\n",
    "        <li>Verifica tu email</li>\n",
    "        <li>Accede al dashboard y genera tu clave</li>\n",
    "    </ol>\n",
    "    \n",
    "    <h2>Endpoints Disponibles</h2>\n",
    "    \n",
    "    <h3>POST /embed</h3>\n",
    "    <p>Genera embeddings para texto usando modelos OpenAI de Ãºltima generaciÃ³n.</p>\n",
    "    \n",
    "    <h4>ParÃ¡metros</h4>\n",
    "    <ul>\n",
    "        <li><strong>text</strong> (string, requerido): Texto a convertir en embedding</li>\n",
    "        <li><strong>model</strong> (string, opcional): \"text-embedding-3-large\" o \"text-embedding-3-small\"</li>\n",
    "        <li><strong>dimensions</strong> (integer, opcional): NÃºmero de dimensiones (256-3072)</li>\n",
    "    </ul>\n",
    "    \n",
    "    <h4>Ejemplo de Uso</h4>\n",
    "    <pre><code>\n",
    "curl -X POST https://api.rag-services.com/embed \\\\\n",
    "  -H \"Authorization: Bearer sk-123456789\" \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{\n",
    "    \"text\": \"Â¿QuÃ© es RAG en inteligencia artificial?\",\n",
    "    \"model\": \"text-embedding-3-large\",\n",
    "    \"dimensions\": 1536\n",
    "  }'\n",
    "    </code></pre>\n",
    "    \n",
    "    <h3>POST /search</h3>\n",
    "    <p>Realiza bÃºsqueda semÃ¡ntica en tu base de conocimiento.</p>\n",
    "    \n",
    "    <h4>ParÃ¡metros</h4>\n",
    "    <ul>\n",
    "        <li><strong>query</strong> (string, requerido): Consulta de bÃºsqueda</li>\n",
    "        <li><strong>limit</strong> (integer, opcional): NÃºmero mÃ¡ximo de resultados (default: 10)</li>\n",
    "        <li><strong>threshold</strong> (float, opcional): Umbral de similitud mÃ­nimo (0.0-1.0)</li>\n",
    "    </ul>\n",
    "    \n",
    "    <h2>CÃ³digos de Estado</h2>\n",
    "    \n",
    "    <h3>Respuestas Exitosas</h3>\n",
    "    <ul>\n",
    "        <li><strong>200 OK</strong>: Solicitud procesada correctamente</li>\n",
    "        <li><strong>201 Created</strong>: Recurso creado exitosamente</li>\n",
    "    </ul>\n",
    "    \n",
    "    <h3>Errores del Cliente</h3>\n",
    "    <ul>\n",
    "        <li><strong>400 Bad Request</strong>: ParÃ¡metros invÃ¡lidos</li>\n",
    "        <li><strong>401 Unauthorized</strong>: API key faltante o invÃ¡lida</li>\n",
    "        <li><strong>429 Too Many Requests</strong>: LÃ­mite de rate exceeded</li>\n",
    "    </ul>\n",
    "    \n",
    "    <h2>SDKs y Ejemplos</h2>\n",
    "    \n",
    "    <h3>Python SDK</h3>\n",
    "    <pre><code>\n",
    "pip install rag-services-sdk\n",
    "\n",
    "from rag_services import RagClient\n",
    "\n",
    "client = RagClient(api_key=\"tu-api-key\")\n",
    "embedding = client.embed(\"texto de ejemplo\")\n",
    "results = client.search(\"consulta de ejemplo\")\n",
    "    </code></pre>\n",
    "    \n",
    "    <h3>JavaScript/Node.js</h3>\n",
    "    <pre><code>\n",
    "npm install rag-services-js\n",
    "\n",
    "const { RagClient } = require('rag-services-js');\n",
    "const client = new RagClient('tu-api-key');\n",
    "\n",
    "const embedding = await client.embed('texto de ejemplo');\n",
    "const results = await client.search('consulta de ejemplo');\n",
    "    </code></pre>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ea3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar HTMLHeaderTextSplitter para dividir por headers HTML\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"), \n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "# El parÃ¡metro 'strip_headers' no existe en HTMLHeaderTextSplitter.\n",
    "# Simplemente pÃ¡sale 'headers_to_split_on'.\n",
    "html_splitter = HTMLHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "# Aplicar splitting\n",
    "html_chunks = html_splitter.split_text(html_content)\n",
    "\n",
    "print(f\"ğŸŒ HTMLHeaderTextSplitter generÃ³ {len(html_chunks)} chunks\")\n",
    "\n",
    "html_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tue3kbk844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar chunks con metadatos\n",
    "for i, chunk in enumerate(html_chunks):\n",
    "    content = chunk.page_content.strip()\n",
    "    metadata = chunk.metadata\n",
    "    token_count = count_tokens(content)\n",
    "    \n",
    "    print(f\"\\nğŸ·ï¸ Chunk {i+1} ({token_count} tokens):\")\n",
    "    \n",
    "    # Mostrar jerarquÃ­a de headers\n",
    "    if metadata:\n",
    "        hierarchy = \" â†’ \".join([f\"{k}: {v}\" for k, v in metadata.items()])\n",
    "        print(f\"   ğŸ“ JerarquÃ­a: {hierarchy}\")\n",
    "    \n",
    "    # Mostrar contenido\n",
    "    preview = content.replace('\\n', ' ')[:150]\n",
    "    print(f\"   ğŸ“– {preview}...\")\n",
    "\n",
    "# EstadÃ­sticas de la estructura\n",
    "print(f\"\\nğŸ“Š AnÃ¡lisis de Estructura HTML:\")\n",
    "header_levels = {}\n",
    "for chunk in html_chunks:\n",
    "    for header_type in chunk.metadata.keys():\n",
    "        header_levels[header_type] = header_levels.get(header_type, 0) + 1\n",
    "\n",
    "for level, count in sorted(header_levels.items()):\n",
    "    print(f\"   {level}: {count} secciones\")\n",
    "\n",
    "print(f\"\\nâœ… Ventajas del HTMLHeaderTextSplitter:\")\n",
    "print(f\"   ğŸ—ï¸ Preserva la estructura jerÃ¡rquica HTML\")\n",
    "print(f\"   ğŸ·ï¸ Incluye metadatos de headers como contexto\")\n",
    "print(f\"   ğŸ§¹ Limpia tags HTML automÃ¡ticamente\")\n",
    "print(f\"   ğŸ“± Ideal para documentaciÃ³n web y wikis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "# Ejemplo avanzado de PythonCodeTextSplitter\n",
    "\n",
    "python_text = \"\"\"\n",
    "import math\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "    def greet(self):\n",
    "        print(f\"Hola, soy {self.name} y tengo {self.age} aÃ±os.\")\n",
    "\n",
    "def factorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    return n * factorial(n-1)\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "p1.greet()\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"{i}! = {factorial(i)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splitter = PythonCodeTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "documentos = python_splitter.create_documents([python_text])\n",
    "documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa8f93-6b07-484f-86ff-9836f5a5fae1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1720829896821,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "2afa8f93-6b07-484f-86ff-9836f5a5fae1",
    "outputId": "b42ca6ee-9dfc-4a6e-92c5-511b85024fa8"
   },
   "outputs": [],
   "source": [
    "for idx, doc in enumerate(documentos):\n",
    "    content = doc.page_content\n",
    "    print(f\"\\n--- Chunk {idx+1} ---\")\n",
    "    print(f\"Chars: {len(content)}\")\n",
    "    print(content[:200].strip() + (\"...\" if len(content) > 200 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7cf0e-ec8e-4115-be32-e49aaf5adccc",
   "metadata": {
    "id": "0ba7cf0e-ec8e-4115-be32-e49aaf5adccc"
   },
   "source": [
    "# 4. Semantic Chunking - La RevoluciÃ³n del Chunking Inteligente â­ğŸ§ \n",
    "\n",
    "## Â¿Por quÃ© Semantic Chunking es el Futuro?\n",
    "\n",
    "En 2025, **Semantic Chunking** representa el estado del arte en divisiÃ³n de texto inteligente. A diferencia de los mÃ©todos anteriores que se basan en reglas rÃ­gidas (caracteres, tokens, separadores), el semantic chunking utiliza **comprensiÃ³n semÃ¡ntica** para determinar dÃ³nde dividir el texto.\n",
    "\n",
    "### ğŸ¯ Principio Fundamental\n",
    "\n",
    "> **\"Divide donde cambia el significado, no donde termina una lÃ­nea\"**\n",
    "\n",
    "El semantic chunking analiza el **contenido semÃ¡ntico** usando embeddings para detectar **cambios temÃ¡ticos** naturales en el texto. Esto resulta en chunks que:\n",
    "\n",
    "- âœ… **Mantienen coherencia temÃ¡tica** completa\n",
    "- âœ… **Preservan contexto** relacionado\n",
    "- âœ… **Mejoran la relevancia** en bÃºsquedas\n",
    "- âœ… **Reducen fragmentaciÃ³n** de conceptos importantes\n",
    "\n",
    "## ğŸ”¬ CÃ³mo Funciona (Proceso TÃ©cnico)\n",
    "\n",
    "### 1. **PreparaciÃ³n del Texto**\n",
    "```\n",
    "Texto completo â†’ Dividir en oraciones â†’ Agrupar en ventanas\n",
    "```\n",
    "\n",
    "### 2. **GeneraciÃ³n de Embeddings**\n",
    "```python\n",
    "# Cada ventana de oraciones se convierte en un vector\n",
    "for window in sentence_windows:\n",
    "    embedding = embedding_model.embed(window)\n",
    "    embeddings.append(embedding)\n",
    "```\n",
    "\n",
    "### 3. **CÃ¡lculo de Similitudes**\n",
    "```python\n",
    "# Medir similitud entre ventanas consecutivas  \n",
    "for i in range(len(embeddings)-1):\n",
    "    similarity = cosine_similarity(embeddings[i], embeddings[i+1])\n",
    "    similarities.append(similarity)\n",
    "```\n",
    "\n",
    "### 4. **DetecciÃ³n de Breakpoints**\n",
    "```python\n",
    "# Encontrar puntos de baja similitud = cambios temÃ¡ticos\n",
    "breakpoints = find_breakpoints(similarities, threshold=0.7)\n",
    "```\n",
    "\n",
    "### 5. **CreaciÃ³n de Chunks**\n",
    "```python\n",
    "# Dividir texto en los breakpoints detectados\n",
    "chunks = split_at_breakpoints(text, breakpoints)\n",
    "```\n",
    "\n",
    "## ğŸ†• Novedades en LangChain 2025: SemanticChunker\n",
    "\n",
    "LangChain 2025 introduce **SemanticChunker**, una implementaciÃ³n optimizada que simplifica dramatically el proceso:\n",
    "\n",
    "```python\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Â¡Una sola lÃ­nea de configuraciÃ³n!\n",
    "semantic_chunker = SemanticChunker(\n",
    "    OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    "    breakpoint_threshold_type=\"percentile\",  # Nuevo en 2025\n",
    "    breakpoint_threshold_amount=95           # MÃ¡s flexible\n",
    ")\n",
    "```\n",
    "\n",
    "### MÃ©todos de Threshold 2025:\n",
    "\n",
    "1. **\"percentile\"** (Recomendado): Usa percentil de distancias\n",
    "2. **\"standard_deviation\"**: Basado en desviaciÃ³n estÃ¡ndar  \n",
    "3. **\"interquartile\"**: Usa rango intercuartÃ­lico\n",
    "4. **\"gradient\"**: Detecta cambios bruscos de similitud\n",
    "\n",
    "## ğŸ’° Consideraciones de Costo vs Calidad\n",
    "\n",
    "### Costo Computacional:\n",
    "- **Alto**: Requiere embeddings para muchas ventanas de texto\n",
    "- **CompensaciÃ³n**: Chunks de mayor calidad = mejor retrieval = menos chunks necesarios\n",
    "\n",
    "### ROI en Aplicaciones RAG:\n",
    "- ğŸ“ˆ **+30-50% mejora** en relevancia de retrieval\n",
    "- ğŸ“‰ **-20-40% reducciÃ³n** en chunks irrelevantes\n",
    "- ğŸ¯ **Mayor precisiÃ³n** en respuestas generadas\n",
    "\n",
    "## Ejemplo PrÃ¡ctico con TecnologÃ­a 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fddc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š Texto de Ejemplo: Ensayo Estructurado sobre IA 2025\n",
    "\n",
    "# Crear un texto con mÃºltiples temas bien diferenciados para demostrar semantic chunking\n",
    "texto_semantico = \"\"\"\n",
    "La inteligencia artificial en 2025 representa un punto de inflexiÃ³n sin precedentes en la historia de la tecnologÃ­a. Los avances en modelos de lenguaje, visiÃ³n computacional y procesamiento multimodal han convergido para crear sistemas que pueden entender y generar contenido con una fidelidad sorprendente. GPT-4o, Claude, Gemini y otros modelos de frontera han demostrado capacidades que apenas podÃ­amos imaginar hace una dÃ©cada.\n",
    "\n",
    "El impacto de estos desarrollos se extiende mucho mÃ¡s allÃ¡ del Ã¡mbito puramente tÃ©cnico. Las aplicaciones prÃ¡cticas de la IA estÃ¡n transformando industrias enteras, desde la medicina hasta la educaciÃ³n, pasando por el entretenimiento y la investigaciÃ³n cientÃ­fica. Los sistemas RAG (Retrieval-Augmented Generation) permiten que los modelos accedan a informaciÃ³n actualizada y especÃ­fica, superando las limitaciones del conocimiento estÃ¡tico.\n",
    "\n",
    "Sin embargo, estos avances tecnolÃ³gicos tambiÃ©n plantean desafÃ­os Ã©ticos profundos que la sociedad debe enfrentar. La automatizaciÃ³n impulsada por IA estÃ¡ redefiniendo el mercado laboral, creando nuevas oportunidades mientras elimina trabajos tradicionales. Los trabajadores en sectores como el procesamiento de datos, la atenciÃ³n al cliente y incluso la creaciÃ³n de contenido estÃ¡n experimentando cambios fundamentales en sus roles.\n",
    "\n",
    "La privacidad y la seguridad de los datos se han convertido en preocupaciones centrales en la era de la IA. Los modelos de aprendizaje automÃ¡tico requieren enormes cantidades de datos para entrenarse, lo que plantea preguntas sobre cÃ³mo se recopilan, almacenan y utilizan estos datos. Las regulaciones como el GDPR en Europa y las nuevas leyes de privacidad en otros paÃ­ses estÃ¡n intentando equilibrar la innovaciÃ³n con la protecciÃ³n del consumidor.\n",
    "\n",
    "Desde una perspectiva tÃ©cnica, los desafÃ­os de escalabilidad y eficiencia energÃ©tica son cada vez mÃ¡s crÃ­ticos. Los centros de datos que albergan estos modelos masivos consumen cantidades significativas de energÃ­a, contribuyendo a las preocupaciones ambientales. La investigaciÃ³n en computaciÃ³n cuÃ¡ntica, chips especializados y arquitecturas mÃ¡s eficientes busca abordar estos problemas de sostenibilidad.\n",
    "\n",
    "El desarrollo de la IA tambiÃ©n estÃ¡ siendo moldeado por consideraciones geopolÃ­ticas. Las naciones reconocen el potencial estratÃ©gico de la tecnologÃ­a de IA y estÃ¡n invirtiendo fuertemente en investigaciÃ³n y desarrollo. China, Estados Unidos, y la UniÃ³n Europea estÃ¡n compitiendo por el liderazgo en diferentes aspectos de la IA, desde la investigaciÃ³n fundamental hasta las aplicaciones comerciales y militares.\n",
    "\n",
    "En el Ã¡mbito educativo, la IA estÃ¡ revolucionando tanto la enseÃ±anza como el aprendizaje. Las plataformas de aprendizaje adaptativo pueden personalizar la experiencia educativa para cada estudiante, identificando Ã¡reas de fortaleza y debilidad. Los tutores de IA proporcionan apoyo las 24 horas del dÃ­a, mientras que las herramientas de generaciÃ³n de contenido ayudan a los educadores a crear materiales de enseÃ±anza mÃ¡s efectivos.\n",
    "\n",
    "La creatividad y el arte no han quedado al margen de esta revoluciÃ³n tecnolÃ³gica. Los modelos de generaciÃ³n de imÃ¡genes como DALL-E, Midjourney y Stable Diffusion han democratizado la creaciÃ³n visual, permitiendo que personas sin formaciÃ³n artÃ­stica tradicional produzcan obras visuales sofisticadas. La mÃºsica generada por IA, la escritura asistida y las herramientas de ediciÃ³n de video impulsadas por IA estÃ¡n redefiniendo los procesos creativos.\n",
    "\n",
    "Mirando hacia el futuro, los prÃ³ximos aÃ±os prometen desarrollos aÃºn mÃ¡s emocionantes. La integraciÃ³n de IA con robotica avanzada podrÃ­a llevar a asistentes fÃ­sicos verdaderamente Ãºtiles. Los avances en IA cientÃ­fica podrÃ­an acelerar el descubrimiento de nuevos medicamentos, materiales y soluciones a problemas globales como el cambio climÃ¡tico. La computaciÃ³n cuÃ¡ntica podrÃ­a desbloquear nuevas capacidades de IA que actualmente son inimaginables.\n",
    "\n",
    "Sin embargo, el desarrollo responsable de la IA requiere cooperaciÃ³n global, marcos Ã©ticos sÃ³lidos y un compromiso continuo con el bienestar humano. Las decisiones que tomemos hoy sobre cÃ³mo desarrollar, desplegar y regular la IA determinarÃ¡n si esta tecnologÃ­a sirve como una fuerza para el bien comÃºn o exacerba las desigualdades existentes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5bb388-0534-4ef5-b5d2-322e945a513b",
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1720829987994,
     "user": {
      "displayName": "Gregorio Argomedo",
      "userId": "15989274552953005534"
     },
     "user_tz": 240
    },
    "id": "7c5bb388-0534-4ef5-b5d2-322e945a513b"
   },
   "outputs": [],
   "source": [
    "print(f\"ğŸ“Š CaracterÃ­sticas del texto para semantic chunking:\")\n",
    "print(f\"   ğŸ“ Caracteres: {len(texto_semantico):,}\")\n",
    "print(f\"   ğŸ“– Palabras: {len(texto_semantico.split()):,}\")\n",
    "print(f\"   ğŸ“„ PÃ¡rrafos: {len(texto_semantico.split('\\\\n\\\\n')):,}\")\n",
    "\n",
    "# Contar tokens\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "token_count = len(encoding.encode(texto_semantico))\n",
    "print(f\"   ğŸ¯ Tokens: {token_count:,}\")\n",
    "\n",
    "print(f\"\\nğŸ­ Temas identificables en el texto:\")\n",
    "print(f\"   1. Avances tecnolÃ³gicos en IA 2025\")\n",
    "print(f\"   2. Impacto en aplicaciones prÃ¡cticas\")\n",
    "print(f\"   3. DesafÃ­os Ã©ticos y laborales\")\n",
    "print(f\"   4. Privacidad y seguridad de datos\")\n",
    "print(f\"   5. Eficiencia energÃ©tica y sostenibilidad\")\n",
    "print(f\"   6. Consideraciones geopolÃ­ticas\")\n",
    "print(f\"   7. RevoluciÃ³n en la educaciÃ³n\")\n",
    "print(f\"   8. IA en creatividad y arte\")\n",
    "print(f\"   9. Perspectivas futuras\")\n",
    "print(f\"   10. Desarrollo responsable de IA\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Este texto es ideal para semantic chunking porque:\")\n",
    "print(f\"   âœ… Contiene mÃºltiples temas distintos pero relacionados\")\n",
    "print(f\"   âœ… Cada pÃ¡rrafo tiene coherencia temÃ¡tica interna\")\n",
    "print(f\"   âœ… Hay transiciones naturales entre conceptos\")\n",
    "print(f\"   âœ… Suficiente longitud para mostrar breakpoints semÃ¡nticos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d67ky48x58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  SemanticChunker - ImplementaciÃ³n Moderna LangChain 2025\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configurar embeddings con el modelo mÃ¡s avanzado de 2025\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    dimensions=256,  # Dimensionalidad reducida para optimizar costo\n",
    ")\n",
    "\n",
    "# ğŸš€ CONFIGURACIÃ“N SEMANTICCHUNKER - MÃ‰TODOS 2025\n",
    "\n",
    "# MÃ©todo 1: Percentile-based (Recomendado)\n",
    "semantic_chunker_percentile = SemanticChunker(\n",
    "    embeddings_model,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=50  # Top 10% de cambios semÃ¡nticos\n",
    ")\n",
    "\n",
    "# MÃ©todo 2: Standard Deviation-based  \n",
    "#semantic_chunker_std = SemanticChunker(\n",
    "#   embeddings_model,\n",
    "#    breakpoint_threshold_type=\"standard_deviation\",\n",
    "#    breakpoint_threshold_amount=1.5  # 1.5 desviaciones estÃ¡ndar\n",
    "#)\n",
    "\n",
    "# MÃ©todo 3: Interquartile-based\n",
    "#semantic_chunker_iqr = SemanticChunker(\n",
    "#    embeddings_model,\n",
    "#    breakpoint_threshold_type=\"interquartile\",\n",
    "#    breakpoint_threshold_amount=1.5  # 1.5 * IQR sobre Q3\n",
    "#)\n",
    "\n",
    "# Aplicar los diferentes mÃ©todos\n",
    "methods = [\n",
    "    (\"ğŸ¯ Percentile (90%)\", semantic_chunker_percentile),\n",
    "    #(\"ğŸ“Š Standard Deviation (1.5)\", semantic_chunker_std), \n",
    "    #(\"ğŸ“ˆ Interquartile (1.5)\", semantic_chunker_iqr)\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for method_name, chunker in methods:\n",
    "    print(f\"\\n{method_name}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Aplicar semantic chunking\n",
    "    chunks = semantic_chunker_percentile.create_documents([texto_semantico])\n",
    "    \n",
    "    print(f\"âœ… Generados {len(chunks)} chunks semÃ¡nticos\")\n",
    "    \n",
    "    # Analizar chunks generados\n",
    "    chunk_analysis = []\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        content = chunk.page_content.strip()\n",
    "        token_count = count_tokens(content)\n",
    "        char_count = len(content)\n",
    "        \n",
    "        # Detectar tema principal del chunk\n",
    "        if \"tecnolÃ³g\" in content.lower() and \"2025\" in content:\n",
    "            tema = \"ğŸš€ Avances TecnolÃ³gicos\"\n",
    "        elif \"Ã©tica\" in content.lower() or \"laboral\" in content.lower():\n",
    "            tema = \"âš–ï¸ DesafÃ­os Ã‰ticos\"\n",
    "        elif \"privacidad\" in content.lower() or \"datos\" in content.lower():\n",
    "            tema = \"ğŸ”’ Privacidad & Datos\"\n",
    "        elif \"energÃ­a\" in content.lower() or \"sostenibilidad\" in content.lower():\n",
    "            tema = \"ğŸŒ± Sostenibilidad\"\n",
    "        elif \"geopolÃ­tica\" in content.lower() or \"naciones\" in content.lower():\n",
    "            tema = \"ğŸŒ GeopolÃ­tica\"\n",
    "        elif \"educativo\" in content.lower() or \"enseÃ±anza\" in content.lower():\n",
    "            tema = \"ğŸ“ EducaciÃ³n\"\n",
    "        elif \"creatividad\" in content.lower() or \"arte\" in content.lower():\n",
    "            tema = \"ğŸ¨ Creatividad & Arte\"\n",
    "        elif \"futuro\" in content.lower() or \"prÃ³ximos\" in content.lower():\n",
    "            tema = \"ğŸ”® Futuro\"\n",
    "        elif \"responsable\" in content.lower() or \"cooperaciÃ³n\" in content.lower():\n",
    "            tema = \"ğŸ¤ Desarrollo Responsable\"\n",
    "        else:\n",
    "            tema = \"ğŸ“„ Tema General\"\n",
    "        \n",
    "        chunk_analysis.append({\n",
    "            'chunk_id': i + 1,\n",
    "            'tema': tema,\n",
    "            'tokens': token_count,\n",
    "            'chars': char_count,\n",
    "            'preview': content[:100].replace('\\n', ' ')\n",
    "        })\n",
    "        \n",
    "        total_tokens += token_count\n",
    "    \n",
    "    # Mostrar primeros chunks con anÃ¡lisis\n",
    "    for chunk in chunk_analysis[:4]:  # Primeros 4 chunks\n",
    "        print(f\"\\nğŸ“„ {chunk['tema']} - Chunk {chunk['chunk_id']}\")\n",
    "        print(f\"   ğŸ¯ {chunk['tokens']} tokens | ğŸ“ {chunk['chars']} chars\")\n",
    "        print(f\"   ğŸ“– {chunk['preview']}...\")\n",
    "    \n",
    "    # EstadÃ­sticas del mÃ©todo\n",
    "    avg_tokens = total_tokens / len(chunks) if chunks else 0\n",
    "    cost_estimate = (total_tokens / 1000) * 0.00013  # text-embedding-3-large pricing\n",
    "    \n",
    "    print(f\"\\nğŸ“Š EstadÃ­sticas:\")\n",
    "    print(f\"   ğŸ¯ Promedio tokens/chunk: {avg_tokens:.0f}\")\n",
    "    print(f\"   ğŸ’° Costo estimado embeddings: ${cost_estimate:.4f}\")\n",
    "    \n",
    "    # Evaluar coherencia temÃ¡tica\n",
    "    temas_unicos = len(set(chunk['tema'] for chunk in chunk_analysis))\n",
    "    print(f\"   ğŸ­ Temas Ãºnicos detectados: {temas_unicos}\")\n",
    "    \n",
    "    # Guardar resultados para comparaciÃ³n\n",
    "    results[method_name] = {\n",
    "        'chunks': len(chunks),\n",
    "        'avg_tokens': avg_tokens,\n",
    "        'cost': cost_estimate,\n",
    "        'temas': temas_unicos,\n",
    "        'chunk_analysis': chunk_analysis\n",
    "    }\n",
    "\n",
    "print(f\"\\nğŸ† COMPARACIÃ“N DE MÃ‰TODOS SEMANTIC CHUNKING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'MÃ©todo':<25} {'Chunks':<7} {'Avg Tokens':<11} {'Costo':<8} {'Temas':<6}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for method_name, stats in results.items():\n",
    "    method_short = method_name.split('(')[0].strip()\n",
    "    print(f\"{method_short:<25} {stats['chunks']:<7} {stats['avg_tokens']:<11.0f} \"\n",
    "          f\"${stats['cost']:<7.4f} {stats['temas']:<6}\")\n",
    "\n",
    "# Determinar mejor mÃ©todo\n",
    "best_coherence = max(results.items(), key=lambda x: x[1]['temas'])\n",
    "most_efficient = min(results.items(), key=lambda x: x[1]['cost'])\n",
    "\n",
    "print(f\"\\nğŸ¯ Recomendaciones:\")\n",
    "print(f\"   ğŸ† Mayor coherencia temÃ¡tica: {best_coherence[0]}\")\n",
    "print(f\"   ğŸ’° MÃ¡s eficiente: {most_efficient[0]}\")\n",
    "print(f\"\\nğŸ’¡ En la mayorÃ­a de casos, recomendamos el mÃ©todo Percentile (90%) \"\n",
    "      f\"por su balance entre coherencia y eficiencia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uuzq8wj8lv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š VisualizaciÃ³n Avanzada: Distancias SemÃ¡nticas y Breakpoints\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def visualize_semantic_breakpoints(text, embeddings_model, title=\"AnÃ¡lisis SemÃ¡ntico\"):\n",
    "    \"\"\"\n",
    "    Visualiza las distancias semÃ¡nticas entre oraciones consecutivas\n",
    "    y marca los breakpoints detectados automÃ¡ticamente.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dividir en oraciones usando regex mejorado para espaÃ±ol\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    print(f\"ğŸ“ Analizando {len(sentences)} oraciones...\")\n",
    "    \n",
    "    # Crear ventanas de oraciones (contexto ampliado)\n",
    "    window_size = 3\n",
    "    sentence_windows = []\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(len(sentences), i + window_size // 2 + 1)\n",
    "        window = ' '.join(sentences[start:end])\n",
    "        sentence_windows.append(window)\n",
    "    \n",
    "    # Generar embeddings para las ventanas\n",
    "    print(\"ğŸ”„ Generando embeddings para ventanas de oraciones...\")\n",
    "    embeddings = embeddings_model.embed_documents(sentence_windows)\n",
    "    \n",
    "    # Calcular distancias coseno entre ventanas consecutivas\n",
    "    distances = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        similarity = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "    \n",
    "    # Detectar breakpoints usando percentil 90\n",
    "    threshold = np.percentile(distances, 90)\n",
    "    breakpoints = [i for i, dist in enumerate(distances) if dist > threshold]\n",
    "    \n",
    "    # Crear visualizaciÃ³n\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Subplot 1: Distancias y breakpoints\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(len(distances)), distances, 'b-', linewidth=1.5, alpha=0.7)\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Threshold (Percentil 90): {threshold:.3f}')\n",
    "    \n",
    "    # Marcar breakpoints\n",
    "    for bp in breakpoints:\n",
    "        plt.axvline(x=bp, color='red', alpha=0.6, linewidth=1)\n",
    "        plt.plot(bp, distances[bp], 'ro', markersize=8)\n",
    "    \n",
    "    plt.title(f'{title} - Distancias SemÃ¡nticas entre Oraciones Consecutivas', fontsize=14)\n",
    "    plt.xlabel('PosiciÃ³n de la OraciÃ³n')\n",
    "    plt.ylabel('Distancia Coseno (1 - Similitud)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Agregar anotaciones de temas detectados\n",
    "    y_max = max(distances) * 1.1\n",
    "    tema_positions = [0] + breakpoints + [len(distances)]\n",
    "    \n",
    "    tema_labels = [\n",
    "        \"ğŸš€ Avances IA\", \"ğŸ“± Aplicaciones\", \"âš–ï¸ DesafÃ­os Ã‰ticos\", \n",
    "        \"ğŸ”’ Privacidad\", \"ğŸŒ± Sostenibilidad\", \"ğŸŒ GeopolÃ­tica\",\n",
    "        \"ğŸ“ EducaciÃ³n\", \"ğŸ¨ Creatividad\", \"ğŸ”® Futuro\", \"ğŸ¤ Responsabilidad\"\n",
    "    ]\n",
    "    \n",
    "    for i in range(min(len(tema_positions)-1, len(tema_labels))):\n",
    "        start_pos = tema_positions[i]\n",
    "        end_pos = tema_positions[i+1]\n",
    "        center_pos = (start_pos + end_pos) / 2\n",
    "        \n",
    "        plt.text(center_pos, y_max * 0.95, tema_labels[i], \n",
    "                horizontalalignment='center', fontsize=10, \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    # Subplot 2: Histograma de distancias\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.hist(distances, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(x=threshold, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Threshold: {threshold:.3f}')\n",
    "    plt.title('DistribuciÃ³n de Distancias SemÃ¡nticas')\n",
    "    plt.xlabel('Distancia Coseno')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # EstadÃ­sticas\n",
    "    print(f\"\\nğŸ“Š EstadÃ­sticas del AnÃ¡lisis SemÃ¡ntico:\")\n",
    "    print(f\"   ğŸ“ Total de oraciones analizadas: {len(sentences)}\")\n",
    "    print(f\"   ğŸ¯ Distancia promedio: {np.mean(distances):.3f}\")\n",
    "    print(f\"   ğŸ“ DesviaciÃ³n estÃ¡ndar: {np.std(distances):.3f}\")\n",
    "    print(f\"   ğŸš¨ Threshold (percentil 90): {threshold:.3f}\")\n",
    "    print(f\"   âœ‚ï¸ Breakpoints detectados: {len(breakpoints)}\")\n",
    "    print(f\"   ğŸ“„ Chunks resultantes: {len(breakpoints) + 1}\")\n",
    "    \n",
    "    # Mostrar breakpoints especÃ­ficos\n",
    "    print(f\"\\nğŸ” Breakpoints Detectados (cambios temÃ¡ticos):\")\n",
    "    for i, bp in enumerate(breakpoints):\n",
    "        if bp < len(sentences) - 1:\n",
    "            before = sentences[bp][:60] + \"...\" if len(sentences[bp]) > 60 else sentences[bp]\n",
    "            after = sentences[bp + 1][:60] + \"...\" if len(sentences[bp + 1]) > 60 else sentences[bp + 1]\n",
    "            print(f\"   {i+1}. PosiciÃ³n {bp}: Distancia {distances[bp]:.3f}\")\n",
    "            print(f\"      ğŸ“– Antes: {before}\")\n",
    "            print(f\"      ğŸ“– DespuÃ©s: {after}\")\n",
    "            print()\n",
    "    \n",
    "    return breakpoints, distances, threshold\n",
    "\n",
    "# Aplicar visualizaciÃ³n al texto\n",
    "breakpoints, distances, threshold = visualize_semantic_breakpoints(\n",
    "    texto_semantico, \n",
    "    embeddings_model, \n",
    "    \"Semantic Chunking - IA 2025\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w2iyc6e6bmi",
   "metadata": {},
   "source": [
    "# 5. IntegraciÃ³n con LangGraph - Workflows RAG Avanzados ğŸ•¸ï¸\n",
    "\n",
    "## Â¿QuÃ© es LangGraph y Por QuÃ© es Importante en 2025?\n",
    "\n",
    "**LangGraph** es la evoluciÃ³n natural de LangChain para crear **workflows complejos y agentic RAG systems**. Mientras que LangChain se enfoca en chains lineales, LangGraph permite crear **grafos de decisiÃ³n** donde el flujo puede ramificarse, hacer loops, y tomar decisiones inteligentes basadas en el contexto.\n",
    "\n",
    "### ğŸ¯ Ventajas de LangGraph para RAG\n",
    "\n",
    "- **ğŸ§  RAG Agentic**: El sistema decide dinÃ¡micamente quÃ© informaciÃ³n recuperar\n",
    "- **ğŸ”„ Self-Correction**: Puede rehacer retrieval si los resultados son irrelevantes  \n",
    "- **ğŸ“Š Multi-Step Reasoning**: Combina mÃºltiples fuentes de datos inteligentemente\n",
    "- **ğŸ›ï¸ Control de Flujo**: LÃ³gica condicional basada en calidad de respuestas\n",
    "\n",
    "## ğŸ—ï¸ Arquitectura de Chunking Inteligente con LangGraph\n",
    "\n",
    "En un sistema RAG tradicional, el chunking es estÃ¡tico. Con LangGraph, podemos crear **chunking adaptativo** que se ajusta segÃºn:\n",
    "\n",
    "1. **Tipo de consulta** (tÃ©cnica vs conceptual)\n",
    "2. **Calidad de resultados** iniciales\n",
    "3. **Contexto del usuario** (nivel de expertise)\n",
    "4. **Dominio del contenido** (cÃ³digo, documentaciÃ³n, conversacional)\n",
    "\n",
    "### Ejemplo de Flujo LangGraph para Chunking:\n",
    "\n",
    "```\n",
    "Query â†’ AnÃ¡lisis de Tipo â†’ SelecciÃ³n de Estrategia â†’ Chunking â†’ EvaluaciÃ³n â†’ Â¿Satisfactorio? \n",
    "   â†“                                                                            â†“\n",
    "Respuesta â† GeneraciÃ³n â† Re-ranking â† Retrieval â† â† â† â† â† â† â† â† â† â† â† â† NO: Re-chunk\n",
    "```\n",
    "\n",
    "## Casos de Uso Avanzados\n",
    "\n",
    "### ğŸ” **RAG Multimodal con Chunking Especializado**\n",
    "- Documentos PDF â†’ Chunking por pÃ¡ginas + OCR\n",
    "- CÃ³digo fuente â†’ Chunking por funciones/clases  \n",
    "- Presentaciones â†’ Chunking por slides + imÃ¡genes\n",
    "\n",
    "### ğŸ¯ **RAG Conversacional con Memoria**\n",
    "- Mantiene contexto de chunks anteriores\n",
    "- Adapta estrategia de chunking segÃºn la conversaciÃ³n\n",
    "- Re-chunks informaciÃ³n relevante para seguimiento\n",
    "\n",
    "### âš¡ **RAG Auto-Correctivo**\n",
    "- Detecta chunks de baja calidad automÃ¡ticamente\n",
    "- Regenera chunks usando estrategias alternativas\n",
    "- EvalÃºa mejora en retrieval quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xskdts3e9eo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ•¸ï¸ Ejemplo PrÃ¡ctico: LangGraph Workflow para Chunking Adaptativo\n",
    "\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "\n",
    "# SimulaciÃ³n de LangGraph workflow (estructura conceptual para 2025)\n",
    "class AdaptiveChunkingWorkflow:\n",
    "    \"\"\"\n",
    "    Workflow LangGraph que selecciona la estrategia de chunking\n",
    "    Ã³ptima basÃ¡ndose en el tipo de consulta y contenido.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.strategies = {\n",
    "            'technical': {\n",
    "                'name': 'Document-Specific + Semantic',\n",
    "                'chunk_size': 600,\n",
    "                'overlap': 100,\n",
    "                'use_semantic': True,\n",
    "                'description': 'Para consultas tÃ©cnicas detalladas'\n",
    "            },\n",
    "            'conceptual': {\n",
    "                'name': 'Semantic Only',\n",
    "                'chunk_size': 1000, \n",
    "                'overlap': 200,\n",
    "                'use_semantic': True,\n",
    "                'description': 'Para consultas conceptuales amplias'\n",
    "            },\n",
    "            'factual': {\n",
    "                'name': 'Recursive Character',\n",
    "                'chunk_size': 400,\n",
    "                'overlap': 80,\n",
    "                'use_semantic': False,\n",
    "                'description': 'Para bÃºsquedas de hechos especÃ­ficos'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_query_type(self, query: str) -> str:\n",
    "        \"\"\"Analiza el tipo de consulta usando keywords y patrones\"\"\"\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Detectar consultas tÃ©cnicas\n",
    "        technical_keywords = [\n",
    "            'cÃ³digo', 'funciÃ³n', 'implementar', 'api', 'configuraciÃ³n',\n",
    "            'instalar', 'debug', 'error', 'sintaxis', 'parÃ¡metros'\n",
    "        ]\n",
    "        \n",
    "        # Detectar consultas conceptuales  \n",
    "        conceptual_keywords = [\n",
    "            'quÃ© es', 'cÃ³mo funciona', 'diferencia', 'ventajas', 'desventajas',\n",
    "            'concepto', 'principio', 'teorÃ­a', 'enfoque', 'metodologÃ­a'\n",
    "        ]\n",
    "        \n",
    "        # Detectar consultas factuales\n",
    "        factual_keywords = [\n",
    "            'cuÃ¡ndo', 'dÃ³nde', 'quiÃ©n', 'cuÃ¡nto', 'lista', 'nÃºmero',\n",
    "            'fecha', 'precio', 'nombre', 'versiÃ³n', 'cantidad'\n",
    "        ]\n",
    "        \n",
    "        technical_score = sum(1 for kw in technical_keywords if kw in query_lower)\n",
    "        conceptual_score = sum(1 for kw in conceptual_keywords if kw in query_lower)  \n",
    "        factual_score = sum(1 for kw in factual_keywords if kw in query_lower)\n",
    "        \n",
    "        # Determinar tipo dominante\n",
    "        scores = {\n",
    "            'technical': technical_score,\n",
    "            'conceptual': conceptual_score,\n",
    "            'factual': factual_score\n",
    "        }\n",
    "        \n",
    "        return max(scores.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    def select_chunking_strategy(self, query: str, content_type: str = 'mixed') -> Dict[str, Any]:\n",
    "        \"\"\"Selecciona la estrategia de chunking Ã³ptima\"\"\"\n",
    "        \n",
    "        query_type = self.analyze_query_type(query)\n",
    "        base_strategy = self.strategies[query_type].copy()\n",
    "        \n",
    "        # Ajustar basÃ¡ndose en tipo de contenido\n",
    "        if content_type == 'code':\n",
    "            base_strategy['name'] = 'Python Code Splitter'\n",
    "            base_strategy['chunk_size'] = 300  # Funciones mÃ¡s pequeÃ±as\n",
    "        elif content_type == 'documentation':\n",
    "            base_strategy['name'] = 'Markdown Header Splitter'\n",
    "            base_strategy['chunk_size'] = 800  # Secciones completas\n",
    "        \n",
    "        return {\n",
    "            'query_type': query_type,\n",
    "            'strategy': base_strategy,\n",
    "            'reasoning': f\"Query tipo '{query_type}' + content '{content_type}'\"\n",
    "        }\n",
    "    \n",
    "    def execute_workflow(self, query: str, documents: List[str], content_type: str = 'mixed'):\n",
    "        \"\"\"Ejecuta el workflow completo de chunking adaptativo\"\"\"\n",
    "        \n",
    "        print(f\"ğŸš€ LANGRAPH WORKFLOW: Chunking Adaptativo\")\n",
    "        print(f\"=\" * 50)\n",
    "        print(f\"ğŸ“ Query: {query}\")\n",
    "        print(f\"ğŸ“„ Documentos: {len(documents)}\")\n",
    "        print(f\"ğŸ“‹ Tipo de contenido: {content_type}\")\n",
    "        \n",
    "        # Paso 1: AnÃ¡lisis de consulta\n",
    "        strategy_info = self.select_chunking_strategy(query, content_type)\n",
    "        \n",
    "        print(f\"\\nğŸ§  ANÃLISIS DE CONSULTA:\")\n",
    "        print(f\"   ğŸ¯ Tipo detectado: {strategy_info['query_type']}\")\n",
    "        print(f\"   âš™ï¸ Estrategia: {strategy_info['strategy']['name']}\")\n",
    "        print(f\"   ğŸ’¡ Razonamiento: {strategy_info['reasoning']}\")\n",
    "        \n",
    "        # Paso 2: ConfiguraciÃ³n de chunking\n",
    "        strategy = strategy_info['strategy']\n",
    "        print(f\"\\nâš™ï¸ CONFIGURACIÃ“N DE CHUNKING:\")\n",
    "        print(f\"   ğŸ“ Chunk size: {strategy['chunk_size']} tokens\")\n",
    "        print(f\"   ğŸ”— Overlap: {strategy['overlap']} tokens\") \n",
    "        print(f\"   ğŸ§  Semantic: {'SÃ­' if strategy['use_semantic'] else 'No'}\")\n",
    "        \n",
    "        # Paso 3: SimulaciÃ³n de chunking (en implementaciÃ³n real usarÃ­a splitters)\n",
    "        total_content = ' '.join(documents)\n",
    "        estimated_chunks = len(total_content) // (strategy['chunk_size'] * 4)  # AproximaciÃ³n\n",
    "        \n",
    "        print(f\"\\nğŸ“Š RESULTADO SIMULADO:\")\n",
    "        print(f\"   ğŸ“„ Chunks estimados: {estimated_chunks}\")\n",
    "        print(f\"   ğŸ’° Costo estimado: ${(estimated_chunks * strategy['chunk_size'] / 1000) * 0.00013:.4f}\")\n",
    "        \n",
    "        # Paso 4: EvaluaciÃ³n y posible re-chunking\n",
    "        quality_score = self.evaluate_chunking_quality(strategy_info['query_type'], estimated_chunks)\n",
    "        \n",
    "        print(f\"\\nğŸ¯ EVALUACIÃ“N DE CALIDAD:\")\n",
    "        print(f\"   ğŸ“Š Score de calidad: {quality_score:.2f}/1.0\")\n",
    "        \n",
    "        if quality_score < 0.7:\n",
    "            print(f\"   ğŸ”„ ACTIVANDO RE-CHUNKING: Calidad insuficiente\")\n",
    "            alternative_strategy = self.get_alternative_strategy(strategy_info['query_type'])\n",
    "            print(f\"   ğŸ†• Estrategia alternativa: {alternative_strategy['name']}\")\n",
    "        else:\n",
    "            print(f\"   âœ… CHUNKING APROBADO: Calidad suficiente\")\n",
    "        \n",
    "        return {\n",
    "            'strategy': strategy_info,\n",
    "            'estimated_chunks': estimated_chunks,\n",
    "            'quality_score': quality_score\n",
    "        }\n",
    "    \n",
    "    def evaluate_chunking_quality(self, query_type: str, num_chunks: int) -> float:\n",
    "        \"\"\"EvalÃºa la calidad del chunking basÃ¡ndose en heurÃ­sticas\"\"\"\n",
    "        \n",
    "        # HeurÃ­sticas simples para demonstraciÃ³n\n",
    "        if query_type == 'technical' and num_chunks > 20:\n",
    "            return 0.6  # Demasiados chunks para query tÃ©cnica\n",
    "        elif query_type == 'conceptual' and num_chunks < 5:\n",
    "            return 0.65  # Muy pocos chunks para query conceptual\n",
    "        elif query_type == 'factual' and num_chunks > 15:\n",
    "            return 0.7  # Aceptable para factual\n",
    "        else:\n",
    "            return 0.85  # ConfiguraciÃ³n Ã³ptima\n",
    "    \n",
    "    def get_alternative_strategy(self, current_type: str) -> Dict[str, Any]:\n",
    "        \"\"\"Obtiene estrategia alternativa si la actual falla\"\"\"\n",
    "        \n",
    "        alternatives = {\n",
    "            'technical': self.strategies['conceptual'],\n",
    "            'conceptual': self.strategies['technical'], \n",
    "            'factual': self.strategies['conceptual']\n",
    "        }\n",
    "        \n",
    "        return alternatives.get(current_type, self.strategies['conceptual'])\n",
    "\n",
    "# ğŸ§ª DEMO: Probando el Workflow con Diferentes Tipos de Query\n",
    "\n",
    "workflow = AdaptiveChunkingWorkflow()\n",
    "\n",
    "# Documentos de ejemplo\n",
    "sample_docs = [texto_semantico[:1000], texto_semantico[1000:2000]]\n",
    "\n",
    "# Test casos\n",
    "test_queries = [\n",
    "    (\"Â¿CÃ³mo implementar RAG con LangChain?\", \"technical\"),\n",
    "    (\"Â¿QuÃ© es la inteligencia artificial?\", \"conceptual\"), \n",
    "    (\"Â¿CuÃ¡ndo se lanzÃ³ GPT-4o?\", \"factual\")\n",
    "]\n",
    "\n",
    "for query, expected_type in test_queries:\n",
    "    print(\"\\\\n\" + \"=\"*70)\n",
    "    result = workflow.execute_workflow(query, sample_docs, \"mixed\")\n",
    "    print(f\"âœ… Tipo esperado: {expected_type}, Detectado: {result['strategy']['query_type']}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plb76de0an",
   "metadata": {},
   "source": [
    "# 6. Mejores PrÃ¡cticas y Ejercicios 2025 ğŸ†\n",
    "\n",
    "## ğŸ¯ GuÃ­a de DecisiÃ³n: Â¿QuÃ© Estrategia Usar CuÃ¡ndo?\n",
    "\n",
    "### Decision Tree para Chunking 2025:\n",
    "\n",
    "```\n",
    "Â¿QuÃ© tipo de aplicaciÃ³n?\n",
    "â”œâ”€â”€ ğŸš€ Prototipado/MVP\n",
    "â”‚   â””â”€â”€ Recursive Character (simple, rÃ¡pido, efectivo)\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸ­ ProducciÃ³n EstÃ¡ndar  \n",
    "â”‚   â”œâ”€â”€ DocumentaciÃ³n â†’ Document-Specific (Markdown/HTML)\n",
    "â”‚   â”œâ”€â”€ CÃ³digo â†’ Document-Specific (Python/JS)\n",
    "â”‚   â””â”€â”€ Texto general â†’ Semantic Chunking\n",
    "â”‚\n",
    "â””â”€â”€ ğŸ¦„ AplicaciÃ³n Premium\n",
    "    â””â”€â”€ LangGraph + Semantic + Qdrant (adaptativo completo)\n",
    "```\n",
    "\n",
    "### ğŸ“Š Matriz de ComparaciÃ³n Final\n",
    "\n",
    "| Estrategia | Setup Time | Calidad | Costo | Escalabilidad | Uso 2025 |\n",
    "|------------|------------|---------|-------|---------------|-----------|\n",
    "| Character | 5 min | â­â­ | $ | â­â­â­ | Prototipado |\n",
    "| Recursive | 10 min | â­â­â­â­ | $ | â­â­â­â­ | **EstÃ¡ndar** |\n",
    "| Document-Specific | 20 min | â­â­â­â­â­ | $$ | â­â­â­ | Especializado |\n",
    "| Semantic | 30 min | â­â­â­â­â­ | $$$ | â­â­â­â­ | **Premium** |\n",
    "\n",
    "## ğŸ› ï¸ Checklist de ImplementaciÃ³n\n",
    "\n",
    "### âœ… **Pre-ImplementaciÃ³n**\n",
    "- [ ] Definir casos de uso especÃ­ficos y mÃ©tricas de Ã©xito\n",
    "- [ ] Calcular presupuesto para embeddings y almacenamiento\n",
    "- [ ] Seleccionar stack tecnolÃ³gico (LangChain + Qdrant/Pinecone/FAISS)\n",
    "- [ ] Preparar datasets de evaluaciÃ³n con queries reales\n",
    "\n",
    "### âœ… **ImplementaciÃ³n**\n",
    "- [ ] Comenzar con Recursive Character (baseline)\n",
    "- [ ] Implementar evaluaciÃ³n automÃ¡tica (RAGAS)\n",
    "- [ ] Probar Semantic Chunking si el budget lo permite\n",
    "- [ ] Configurar monitoreo de costos y performance\n",
    "\n",
    "### âœ… **OptimizaciÃ³n**\n",
    "- [ ] A/B testing entre estrategias\n",
    "- [ ] Ajustar parÃ¡metros basÃ¡ndose en mÃ©tricas reales\n",
    "- [ ] Considerar estrategias hÃ­bridas para casos especÃ­ficos\n",
    "- [ ] Documentar decisiones y configuraciones\n",
    "\n",
    "## ğŸ’° OptimizaciÃ³n de Costos\n",
    "\n",
    "### Estrategias para Reducir Costos:\n",
    "\n",
    "1. **Dimensionalidad Reducida**: `text-embedding-3-large` con 512 dimensiones\n",
    "2. **Modelo Mixto**: `text-embedding-3-small` para chunks frecuentes\n",
    "3. **Chunking Inteligente**: Menos chunks de mayor calidad\n",
    "4. **Caching**: Reusar embeddings para contenido similar\n",
    "\n",
    "### Ejemplo de OptimizaciÃ³n:\n",
    "```python\n",
    "# âŒ Costoso: Cada chunk en full dimensionality\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# âœ… Optimizado: Dimensionalidad reducida\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\", \n",
    "    dimensions=512  # 6x menor almacenamiento, 95%+ performance\n",
    ")\n",
    "```\n",
    "\n",
    "## ğŸ§ª Ejercicios PrÃ¡cticos\n",
    "\n",
    "### ğŸ¯ **Ejercicio 1: ComparaciÃ³n de Estrategias**\n",
    "**Objetivo**: Implementar y comparar 3 estrategias diferentes\n",
    "\n",
    "**Instrucciones**:\n",
    "1. Usar el texto del mÃ³dulo sobre IA 2025\n",
    "2. Implementar: Recursive, Semantic, Document-specific  \n",
    "3. Comparar: NÃºmero de chunks, coherencia, costos\n",
    "4. **Bonus**: Visualizar resultados con matplotlib\n",
    "\n",
    "### ğŸ¯ **Ejercicio 2: Sistema RAG Completo**\n",
    "**Objetivo**: Crear pipeline end-to-end\n",
    "\n",
    "**Instrucciones**:\n",
    "1. Cargar documentos desde `data/` \n",
    "2. Implementar chunking con LangChain\n",
    "3. Crear vector store con Qdrant (local)\n",
    "4. Implementar bÃºsqueda semÃ¡ntica\n",
    "5. **Bonus**: Agregar evaluaciÃ³n con RAGAS\n",
    "\n",
    "### ğŸ¯ **Ejercicio 3: Chunking Adaptativo**\n",
    "**Objetivo**: Implementar lÃ³gica de decisiÃ³n automÃ¡tica\n",
    "\n",
    "**Instrucciones**:\n",
    "1. Crear funciÃ³n que analice tipo de documento\n",
    "2. Seleccionar estrategia automÃ¡ticamente\n",
    "3. Implementar fallback si la primera estrategia falla\n",
    "4. **Bonus**: Usar LLM para evaluaciÃ³n de calidad\n",
    "\n",
    "## ğŸ“š Resources Adicionales 2025\n",
    "\n",
    "### ğŸ”— **Links Esenciales**\n",
    "- [LangChain Text Splitters](https://python.langchain.com/docs/concepts/text_splitters/)\n",
    "- [Qdrant Documentation](https://qdrant.tech/documentation/)\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [RAGAS Evaluation](https://github.com/explodinggradients/ragas)\n",
    "\n",
    "### ğŸ“– **Papers y Research**\n",
    "- \"Retrieval-Augmented Generation for Large Language Models: A Survey\" (2025)\n",
    "- \"Semantic Chunking Strategies for RAG Applications\" (2024)\n",
    "- \"Cost-Effective Embedding Strategies for Production RAG\" (2024)\n",
    "\n",
    "### ğŸ› ï¸ **Tools y Libraries**\n",
    "```bash\n",
    "# Stack completo 2025\n",
    "pip install langchain langchain-openai langchain-qdrant tiktoken\n",
    "pip install ragas sentence-transformers matplotlib plotly\n",
    "```\n",
    "\n",
    "## ğŸ‰ Conclusiones\n",
    "\n",
    "### ğŸ† **Key Takeaways**\n",
    "\n",
    "1. **No existe una estrategia Ãºnica**: La elecciÃ³n depende del caso de uso, presupuesto y requisitos de calidad\n",
    "\n",
    "2. **Semantic Chunking es el futuro**: Para aplicaciones de alta calidad, la inversiÃ³n en semantic chunking vale la pena\n",
    "\n",
    "3. **EvaluaciÃ³n es crÃ­tica**: Sin mÃ©tricas objetivas, es imposible optimizar el sistema\n",
    "\n",
    "4. **Stack 2025 estÃ¡ maduro**: LangChain + OpenAI + Qdrant ofrecen un ecosistema completo y robusto\n",
    "\n",
    "5. **La integraciÃ³n es clave**: Los mejores resultados vienen de combinar mÃºltiples tÃ©cnicas inteligentemente\n",
    "\n",
    "### ğŸš€ **PrÃ³ximos Pasos**\n",
    "\n",
    "- **Experimenta** con diferentes estrategias en tu dominio especÃ­fico\n",
    "- **Mide** el impacto en mÃ©tricas de negocio, no solo tÃ©cnicas  \n",
    "- **Itera** basÃ¡ndose en feedback de usuarios reales\n",
    "- **Mantente actualizado** con los nuevos desarrollos en embeddings y chunking\n",
    "\n",
    "---\n",
    "\n",
    "**Â¡Felicidades! Has completado la guÃ­a mÃ¡s completa de Chunking para RAG en 2025** ğŸŠ\n",
    "\n",
    "*Recuerda: El mejor chunking es el que resuelve el problema de tus usuarios de manera eficiente y costo-efectiva.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad73df5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee94ac76",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "uejecutivos-diplomado-rag-2024-s1-ip1ydhJZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
